\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage {tikz}
\usetikzlibrary{arrows.meta}
\usepackage{tabularx,booktabs}
\usepackage{algorithmic}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[shortlabels]{enumitem}
\usepackage{slashed}

\usetikzlibrary {positioning}
%\usepackage {xcolor}
\definecolor {processblue}{cmyk}{0.96,0,0,0}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}


% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}


\title{CS2100}
\author{Jia Cheng}
\date{January 2021}

\begin{document}
\maketitle

\section{Definitions}
\begin{itemize}
	\item MSB: Most significant bit
	\item LSB: Least significant bit
\end{itemize}

\section{Conversion Table}
\begin{center}
\begin{tabular}{ c|c|c } 
 	Dec & Hex & Bin\\
	\hline
	0&	0&	0000\\	 	
	1&	1&	0001\\	 	
	2&	2&	0010\\	 	
	3&	3&	0011\\	 	
	4&	4&	0100\\	 	
	5&	5&	0101\\	 	
	6&	6&	0110\\	 	
	7&	7&	0111\\	 	
	8&	8&	1000\\	 	
	9&	9&	1001\\	 	
	10&	a&	1010\\	 	
	11&	b&	1011\\	 	
	12&	c&	1100\\	 	
	13&	d&	1101\\	 	
	14&	e&	1110\\	 	
	15&	f&	1111\\
	\hline
\end{tabular}
\end{center}


\section{Number representations}

Note: The following terminology may be slightly confusing. 
$2s$ complement representation refers to the number representation format.
$2s$ complement of a number $N$ refers to how $-N$ is represented under the $2s$ complement representation.

Ditto for $1s$ complement representation and $1s$ complement of a number $N$.

\subsection{Bs complement}
Given a base $B$, positive integer $n$. The $n$-digit $B$'s complement representation operates modulo $B^n$.

We now denote $Bs$ as $n$-digit $B$'s complement representation.

An interesting observation is that we can map the $Bs$ values, i.e. the \textbf{ordered} set $\{(b_{n-1}\dots b_0) : b_i\in \{0, 1, \dots, B-1\}\}$ to \textbf{any} translation + cyclic permutation of $\mathbb{Z}/B^{n}\mathbb{Z}$.

We now arbitrarily fix the integer $10$, so as to demonstrate the meaning of translation and cyclic permutation.
An example of translation of $\mathbb{Z}/10\mathbb{Z}$ is $\{2,3,4,\dots, 11\}$. In this case, this is a translation of $+2$. An example of cyclic permutation of $\mathbb{Z}/10\mathbb{Z}$ is $\{2,3,4,\dots, 9, 0, 1\}$.

In the case of the regular $8$-bit $2s$ complement representation, we have a combination of translation and cyclic permutation of $\mathbb{Z}/256\mathbb{Z}$ .

\textbf{Example} To show the completely arbitrary nature of such a map from $\{(b_{n-1}\dots b_0) : b_i\in \{0, 1, \dots, B-1\}\}$ to any translation + cyclic permutation of $\mathbb{Z}/B^{n}\mathbb{Z}$, consider the map of $5$-bits to $\mathbb{Z}/32\mathbb{Z}$. Call this representation $R$. Note that under $2s$ complement, the $5$-bits would have been mapped to the ordered set $\{0, 1, \dots, 15, -16, -15, \dots, -1\}$

Then, $31 = (11111)_R$, $-31 \equiv 32 - 31 (\mod 32) = 1 = (00001)_R$.\\
Hence, $31 - 31 = 32 + (-31) = (11111)_R + (00001)_R = (00000)_R = 0$, which is indeed the expected result.

\subsection{Fractional complements}
\subsubsection{(B-1)'s complement}
Suppose a fractional number $N$ has $n$ integer digits and $m$ fractional digits, then $(B-1)$'s complement of $N$ is given by $B^n-B^{-m}-N$.

To see why, simply remove the decimal point by multiplying $N$ by $B^m$ (and then dividing $B^m$).
Hence, \begin{align*}
	B^{-m}(B^{n+m} - 1 - B^mN) = B^n - B^{-m} - N
\end{align*}



\subsection{Sign extension to the left}
\subsubsection{Binary case}
For positive numbers, i.e. numbers with the MSB 0, in both $2s$ and $1s$ complement, it suffices to pad the "left" of the number with $0$'s.

It is particularly easy to mentally prove the sign extension rules for $1s$, since for a negative number $x$, we simply invert it to get positive $-x$, apply the positive sign extension rules (pad with $0$'s), then invert it back. 

Regardless, it is easy to prove the validity of the sign extension by doing summations.

For \textbf{both} 1s and 2s complement representations
\begin{itemize}
	\item Sign extending number with $0$ as MSB: Pad with $0$'s.
	\item Sign extending number with $1$ as MSB: Pad with $1$'s.
	\item Note that the reason why I didn't say positive/negative here is because in $1s$ complement, there is a positive and a negative $0$ value. Referring to the MSB would be the most general.
\end{itemize}

\subsubsection{General case}

\subsection{Sign extension to the right}
i.e. after the decimal point

Technically this is not called sign extension. But I don't know what is the technical term for this, so I'll just call this sign extension to the right for convenience.


1s complement (fractional)
\begin{itemize}
	\item Sign extending number with $0$ as MSB: Pad with $0$'s.
	\item Sign extending number with $1$ as MSB: Pad with $1$'s.
\end{itemize}

2s complement (fractional)
\begin{itemize}
	\item Pad with $0$'s regardless of MSB
\end{itemize}

\subsection{Excess representation}
Excess-n representation of a number $x$ will be the binary representation of $x+n$.

\textbf{Example} Excess-127 representation of $x$ is binary representation of $x+127$.
The correspondence with $\mathbb{Z}/255\mathbb{Z}$ are as follows:\\
0 ... 127 128 ... 255 (mod 255)\\
0 ... 127 -128 ... -1 (2s complement)\\
-127 ... 0 1 ... 128 (excess-127)\\
-128 ... -1 0 ... 127 (excess-128)

\section{High level code to MIPS Assembly}
\subsection{if-elseif-else}

\subsection{switch-case}

\subsection{while}

\subsection{do-while}

\subsection{for}


\subsection{Encoding space problem}
The most general statement of this problem type is as follows: Given an ISA, with $n$ types of instructions, type $T_1, \dots, T_n$. Each instruction type $T_i$ has an opcode of length $l_i$, with $i < j\implies l_i < l_j$. i.e. $l_1 < \dots < l_n$.

Given that the encoding space is fully utilised and that there are at least $1$ of each type of instructions encoded, what is the minimum and maximum number of instructions?

Define $l_0=0$.\\
\textbf{Minimum} 
\begin{align*}
	\sum_{1\leq i\leq n-1}(2^{l_i-l_{i-1}}-1) + 2^{l_n-l_{n-1}}
\end{align*}

I will give an intuitive argument for the minimum case as I'm not too sure how to give a formal proof.\\
We start by allocating space for the shortest instruction $T_1$. For the first $l_1$ bits, combinatorial analysis (product rule) tells us that there are $2^{l_1}$ "outcomes". Since all other instructions are longer than $T_1$, at least $1$ of the first $2^{l_1}$ must be allocated for the other instructions of type $T_2$ to $T_n$. Since we want to minimise the total number of instructions, we will allocated exactly $1$ of the first $2^{l_1}$ to the other instructions and $2^{l_1}-1$ to instruction type $T_1$.\\
We repeat this argument for $T_2$, allocating $2^{l_2-l_1}-1$ of the next $2^{l_2-l_1}$ outcomes (generated by $l_2-l_1$ bits) to instructions of type $T_2$. And so on.\\
Finally, we reach the final instruction type $T_n$. Then the last $l_n-l_{n-1}$ bits generate  $2^{l_n-l_{n-1}}$ outcomes, which we will all allocate to $T_n$.

\textbf{Maximum}
\begin{align*}
	(n-1) + 2^{l_n} - \sum_{1\leq i\leq n-1}2^{l_n-l_i}
\end{align*}

Argument: For $1\leq i\leq n$, define $S_i$ as the set of encoding allocated to instruction type $T_i$. By problem definition, $|S_i|\geq 1$. So obviously, to maximise $\sum_{1\leq i\leq n} |S_i|$, our strategy is to minimise $|S_i|, 1\leq i<n$ so as to maximise $|S_n|$. In particular, we find a way to let $|S_i|=1, \forall 1\leq i<n$.\\
Here we introduce the notion of making encoding "\textit{unavailable}". For instance, when we allocate $l_i$ bits to an instruction of type $T_i$, $i<n$, the remaining $n-i$ bits are "wasted", that is, they are made \textit{unavailable}. So, in a certain sense, we lose $2^{l_n-l_i}$ encodings.

We know that if $\forall 1\leq i,j<n$(WLOG $i\leq j$), then any instruction of type $T_i$ cannot have the same first $i$ bits as an instruction of type $T_j$. This implies that the encodings made \textit{unavailable} by any 2 such instructions are disjoint.\\
To give a concrete example, suppose we have $n=3$, $l_1=2,l_2=4,l_3=5$. \\
Let $(0,0)$ be an encoding allocated for instruction $A$ of type $T_1$. Let $(0,1,0,0)$ be an encoding allocated for instruction $B$ of type $T_2$.\\
Then the encoding made \textit{unavailable} by $A$ is $\{(0,0,0,0,0),\dots, (0,0,1,1,1)\}$. The encoding made \textit{unavailable} by $B$ is $\{(0,1,0,0,0),(0,1,0,0,1)\}$. Notice how these 2 sets are disjoint since no 2 entries across the 2 respective sets can share the first $2=l_1$ bits.

This idea of disjoint sets gives the intuition for why we can keep on subtracting $2^{l_n-l_i}$, overall subtracting away $\sum_{1\leq i\leq n-1}2^{l_n-l_i}$.

Note that the first $(n-1)$ refers to $\sum_{1\leq i< n} |S_i|$, where $\forall 1\leq i<n,|S_i|=1$. That is, we only allocate $1$ encoding for each instruction of type $T_1,\dots,T_{n-1}$.

\section{Boolean Algebra}
\subsection{Minterms and maxterms}
On $n$ variables $x_1,\dots ,x_n$, define the minterms and maxterms as follows: \\
Consider $m[x]$, where $x\in \{0,\dots,2^n-1\}$, $x=(b_1\dots b_n)_2$.\\
Then for each $i$ in $\{1,\dots, n\}$, we have the literal $x_i$ if $b_i=1$, $x_i'$ if $b_i=0$.\\
Another way to remember this is that the product of literals forming $m[x]$ equals $1$ when each $b_i$ is substituted into $x_i/x_i'$. 

Consider $M[x]$, where $x\in \{0,\dots,2^n-1\}$, $x=(b_1\dots b_n)_2$.\\
Then for each $i$ in $\{1,\dots, n\}$, we have the literal $x_i$ if $b_i=0$, $x_i'$ if $b_i=1$.\\
Another way to remember this is that the sum of literals forming $M[x]$ equals $0$ when each $b_i$ is substituted into $x_i/x_i'$. 

In short, when it comes to minterms, negate the $0$'s, when it comes to maxterms, negate the $1$'s

\end{document}

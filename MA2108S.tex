\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}

\newcommand\tab[1][1cm]{\hspace*{#1}}

% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\title{MA2108S (Mathematical Analysis I)}
\author{Jia Cheng}
\date{January 2021}

\begin{document}

\maketitle

\section{The real numbers}
\subsection{$p$-th powers and roots are order preserving ($p > 0$)}
It is very easy to see that for $n\in \mathbb{Z}^+$, taking $n$-th roots on $r>0$ preserves order. That is, $0 < r_1 < r_2 \implies r_1^{\frac{1}{n}} < r_2^\frac{1}{n}$. This can be easily proven by contradiction.

However, later in chapter 3, theorem 3.20(a) uses the idea that given $p>0$, $\frac{1}{\epsilon} < n^p \iff (\frac{1}{\epsilon})^\frac{1}{p} < n$. Here, $p$ is a positive real number, not just a positive integer.


PMA Ex 1.6 gives a definition on rational powers, then real powers of positive real numbers.

Ex 1.6 shows that it makes sense to define rational powers. Now, we show that positive rational powers $q > 0$ are order preserving. Since $q>0$, $q=\frac{m}{n}$ for some $m,n\in \mathbb{N}^+$. Suppose $0<r_1<r_2$.\\
Then $r_1^\frac{1}{n} < r_2^\frac{1}{n}$. Hence, $(r_1^\frac{1}{n})^m < (r_2^\frac{1}{n})^m$. By definition of rational powers, $r_1^q < r_2^q$.\\
So now we have proven that taking rational powers is order preserving. This implies that taking rational roots is also order preserving. To define rational roots, we need to prove that $(r^a)^b=r^{ab}$. Then, $(r^\frac{1}{q})^q=r^1=r$ will be a consequence of this.

To show that real powers are also order preserving, I believe it suffices to consider supremums of sets and show that one supremum is larger than the other. (see definition of real powers in Ex 1.6)

I believe we might also need to prove that $(r^\frac{1}{p})^p$ indeed recovers $r$ for real $p$, or more generally, $(r^a)^b=r^{ab}$.

\subsection{Methods of bounding $b^\frac{1}{n}, b>1$}
In both proofs, we assume $M > 1$

\textbf{Method 1} (Ex 1.7)
In general, $x^n - 1 = (x-1)(x^{n-1} + \dots + 1)$\\
Hence, $b - 1 \geq n(b^\frac{1}{n}-1)$

$b^\frac{1}{n}<M \iff b^\frac{1}{n} - 1 < M - 1$\\
It suffices to show $b^\frac{1}{n}-1 \leq \frac{b-1}{n} < M - 1$\\
The strict inequality will hold iff $n > \frac{b-1}{M-1}$.

\textbf{Method 2}
This uses binomial expansion and Bernoulli's inequality.
$b^\frac{1}{n}<M \iff b < M^n = (1 + (M - 1))^n$\\
It suffices to show $b < 1 + n(M - 1) \leq M^n$.\\
The strict inequality will hold iff $n > \frac{b-1}{M-1}$.


\subsection{Complex Numbers and $\mathbb{R}^2$}
Both $\mathbb{C}$ and $\mathbb{R}^2$ are normed spaces. With respect to metrics, these 2 spaces are essentially equivalent. 

The only difference between $\mathbb{C}$ and $\mathbb{R}^2$ is the inner product defined on them (and also the multiplication operation on complex numbers). For $\mathbb{R}^2$ (and $\mathbb{R}^n$ in general), it is the dot/scalar product. For $\mathbb{C}$ (and $\mathbb{C}^n$), it is the Hermitian product. \\
Regardless, the normed induced by these inner products with respect to $\mathbb{C}$ and $\mathbb{R}^2$ are effectively the same.
A complex number can be represented by a 2-tuple $(a,b)$, and it's norm is given by $a^2+b^2$, which the same for a tuple in $\mathbb{R}^2$.

Hence, many theorems that apply to sequences in $\mathbb{R}^2$ later on also apply to complex number sequences.

\subsection{Dense sets}
(Theorem 1.20)\\
The notion of denseness in Chapter 1 claims that, to prove $S$ is dense in $\mathbb{R}$, it is equivalent to show that $\forall p,q\in \mathbb{R}, p<q, \exists x\in S, p<x<q$.\\
Note that in general, this is not always true for any set.

For instance, consider $\mathbb{Z}$. $\mathbb{Z}$ is dense with respect to itself. However, between $0$ and $1$, there exists no element of $\mathbb{Z}$.

Of course with respect to $\mathbb{R}$, topological denseness of a set $S$ is the same as saying that we can always find another element of $S$ between two elements $p,q$ of $\mathbb{R}$.


\subsection{Linear Algebra}
The following theorem can be developed without the notion of basis, linear independence or dimension. 
We simply need definition of orthogonality.
It is useful when dealing with orthogonality.

Given subspace $U$ of Euclidean vector space $V$ and vector $v\in V$.
Also assume that $U$ is spanned by orthogonal vectors $\{u_1,\dots, u_m\}$.
Then there exists vectors $p\in U$, $n\in V$, where $n$ is orthogonal to $U$.\\
In particular, 
\begin{align*}
	p = \sum_{i=1}^m\frac{v\cdot u_i}{|u_i|^2}u_i,\, n=v-p
\end{align*}
We call $p$ the projection of $v$ on $U$.

This is used in ex 1.18.


\subsection{Strategies}
\subsubsection{To show $\sup A\leq \sup B$}
One of the ways to show this is the following:\\
For each $a\in A$, there exists $b\in B, a \leq b$.\\
Take supremum of RHS to get $a\leq \sup B$.\\
Use definition of supremum to get $\sup A \leq \sup B$.

\subsubsection{To show $\inf A\leq \inf B$}
This mirrors the previous section. But we'll show it, for clarity.\\
For each $b\in B$, there exists $a\in A, a\leq b$.\\
Take infimum of LHS to get $\inf A\leq b$.\\
Use definition of infimum to get $\inf A\leq \inf B$.




\section{Point Set Topology}
\subsection{Countability}
\textbf{Proposition} If there is an injection from $A$ to countable set $B$, then $A$ is at most countable.

An alternative way of saying this is that if there is a 1-1 correspondence between $A$ and some subset of $B$, then $A$ is at most countable.

\subsection{Definitions}
\begin{itemize}
	\item Metric Space $M$
	\item Open ball $B(p,r)=\{q\in M : d(p,q) < r \}$
	\item Boundary of S: $bd(S)=\{p\in S : \forall r > 0, \exists q\in S, \exists q'\in S^c, q,q'\in B(p,r) \}$
	\item Limit points of S: $lim(S)=\{p\in S : \forall r > 0, \exists q\in S, q\neq p, q\in B(p,r) \}$
	\item Interior of S: $int(S)=\{p\in S : \exists r > 0, B(p,r)\subseteq S\}$
\end{itemize}
\subsection{Equivalence of definitions of closed set}
The following definitions of a closed set are equivalent.

\begin{enumerate}
	\item $bd(S)\subseteq S$
	\item $lim(S)\subseteq S$
\end{enumerate}

\textbf{Proof}: Suppose $bd(S)\subseteq S$. 
Let $x\in lim(S)$ and fix some arbitrary $r\in \mathbb{R}^+$.
If $x\in S$, we are done.\\
Otherwise,  $x\not \in S$. As $x$ is a limit point, $\exists q\in S, q\neq x$ such that $q\in B(x,r)$. Now, notice that $x$ is a boundary point of $S$, since $x$ itself is not in S, and $q$ is in S, and both $x,q\in B(x,r)$.\\
But by our initial assumption, we have $x\in bd(S)\subseteq S$. Contradiction.\\
Hence, $x\in S$ and $\lim(S)\subseteq S$.

Conversely, suppose $lim(S)\subseteq S$.\\
Let $x\in bd(S)$ and fix some arbitrary $r\in \mathbb{R}^+$. If $x\in S$, we are done.\\
Otherwise, $x\not \in S$. As $x$ is a boundary point, $\exists q\in S, q' \in S^c$ such that both are in $B(x,r)$. In particular, $q\neq x$ since $x\not \in S$. This says that $x$ is a limit point of $S$.\\
Our initial supposition says that $x\in lim(S)\subseteq S$. Again, we have a contradiction.  

\subsection{Sequential Compactness}
The following 2 notions are equivalent.
\begin{enumerate}
	\item Every infinite subset $E$ of a set $X$ has a limit point in $X$.
	\item $X$ is sequentially compact, that is, for every sequence $\{p_n\}$ in $X$, there exists a subsequence converging to some point of $X$.
\end{enumerate}

To prove (2) using (1), consider 2 cases: a sequence with finite range, and a sequence with infinite range (this case is proven with (1)).

\section{Sequences}
\subsection{Infinite subsets vs sequences}
There is a subtle difference between these 2 concepts. Of course, infinite sets are unordered, whereas sequences are ordered. \\
Additionally, it must be noted that sequences can have repeat elements (with different indices). The range of an infinite sequence may very well be finite. An example would be the sequence $\{i^n\}_{n\in \mathbb{N}}$, with range $\{\pm 1, \pm i\}$.

Also, with regard to infinite sets, we speak of limit points, and with regard to sequences, we speak of subsequence limits. Again, there is a difference between limit points and subsequential limits (also known as accumulation points in some texts).

For example, a point can be a subsequential limit without being a limit point. Consider the earlier defined set $\{i^n\}$. Every element in the range $\{\pm 1, \pm i\}$ is a subsequential limit, but since the range is finite, none of these subsequential limits are limit points of the range.

As a consequence, these 2 propositions say different things:
\begin{itemize}
	\item The set of limit points $E'$ of a set $E$ is closed.
	\item The set of subsequential limits of a sequence $\{p_n\}$ is closed.
\end{itemize}


Similarly, supremum of a finite subset of $\mathbb{R}$ is clearly not a limit point of the set.

\subsubsection{Supremum of closed subset of $\mathbb{R}$ belongs to set}
Like before, we need to consider 2 cases, whether the set if infinite or not.

If the set is finite, the supremum of a set is the maximum of the set, and is clearly an element of the set.\\
If the set is infinite, the supremum is then a limit point of the set, and by definition of "closed", belongs to the set.


\subsection{Subsequential limits in $\mathbb{R}$ and $\pm \infty$}
This discussion is inspired by Rudin 3.15 to 3.17.

Notice that subsequential limits of a real sequence lie in $(-\infty, +\infty)$, i.e. they are finite. $\pm \infty$ are \textbf{not} considered limits.

This is because the definition of a limit $p$ of a sequence $\{p_n\}$ (or a subsequence) involved the notion of distance $d(p_n, p)$ approaching 0. In particular, $d(p_n, p)$ is defined.\\
Clearly, $d(p_n, +\infty)$ or $d(p_n, -\infty)$ are not defined, since metric $d$ is always a real-valued function.

\subsection{The Extended Real Line}
While this was introduced in Chapter 1, it is in Chapter 3 that we see the application of $\mathbb{R} \cup \{\pm \infty\} = [-\infty, +\infty]$.

Properties of the extended reals
\begin{itemize}
	\item The extended real line is no longer a field.
	\item $\mathbb{R}$ has the completeness property, such that for any upper bounded subset $S$ of $\mathbb{R}$, $\sup S$ exists in $\mathbb{R}$. However, $\sup S$ does not exist in $\mathbb{R}$ if $S$ is unbounded.\\
	 In contrast, all subsets $S$ of $[-\infty, +\infty]$ have supremums and infimums. For an upper unbounded set $S$, we would have $\sup S = + \infty$, and for a lower unbounded set $S$, we would have $\inf S = -\infty$. For a bounded set, we can then apply the completeness property of $\mathbb{R}$ to find a supremum/infimum in $\mathbb{R}\subset [-\infty, +\infty]$
\end{itemize}

The fact that all subsets of $[-\infty, +\infty]$ have well-defined supremum and infimum makes Theorem 3.17 in PMA very easy to state. Given the set $E$ of all subsequential limits of a sequence $\{p_n\}$ \textbf{which \textit{additionally} includes $\pm \infty$ if the sequence is unbounded}, we are guaranteed to find $\sup E, \inf E$, without having to talk about special cases (i.e. bounded vs unbounded sequences), which can be troublesome to write.

Of course, if $\sup E$ turns out to be $+\infty$ or $-\infty$, we must be careful. We can't treat the infinities like subsequential limits. For e.g., we can't use a theorem like \textit{the set of subsequential limits is closed}, since $\pm \infty$ do not belong in the set of subsequential limits.\\
To emphasise: $E$ in theorem 3.17 is the union of subsequential limits (which are finite) and possibly $+\infty$ if the sequence is unbounded above, $-\infty$ if the sequence is unbounded below.


\subsection{Multiple definitions of limsup/liminf}
\textbf{Theorem} The following definitions are equivalent:
\begin{enumerate}
	\item $\limsup_{n\rightarrow \infty}x_n=\sup C(x_n)$, where $C(x_n)$ is the set of subsequential limits of $(x_n)$.
	\item $\limsup_{n\rightarrow \infty}x_n=\inf V$, where $V=\{y\in \mathbb{R}:x_n>y \text{ for finitely many } n\}$
	\item $\limsup_{n\rightarrow \infty}x_n=\inf U_n=\lim_{n\rightarrow \infty}U_n$ where $U_n=\sup\{x_m:m\geq n\}$
\end{enumerate}


\subsection{Two proofs of the Squeeze Theorem}
We are given functions $f,g,h$ where $f(x)\leq g(x)\leq h(x)$ in some neighborhood of $x_0$, such that $f(x)\rightarrow L$ and $h(x)\rightarrow L$. Fix $\epsilon > 0$ and we can obtain the following bound for all $0 < |x - x_0| < \delta$ for some $\delta > 0$.
\begin{align*}
	L - \epsilon < f(x) \leq g(x) \leq h(x) < L + \epsilon
\end{align*}
This says that $g(x)\rightarrow L$ as well.

This can easily be adapted to the proof of squeeze theorem for sequences as well.

A second proof of the squeeze theorem that is more in line with Rudin Chapter 3:

Given sequences $\{a_n\}, \{b_n\}, \{c_n\}$ such that $a_n\leq b_n\leq c_n$ for all but finitely many $n$, and $a_n, c_n\rightarrow L$.

Then we have \begin{align*}
	L = \lim a_n = \liminf a_n \leq \liminf b_n \leq \limsup b_n \leq \limsup c_n = \lim c_n = L
\end{align*}
which implies
\begin{align*}
	\liminf b_n = \limsup b_n = L \implies \lim b_n = L
\end{align*}
The last implies is due to the following: For any $\epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. $n\geq N\implies L - \epsilon = \liminf b_n - \epsilon < b_n < \limsup b_n + \epsilon = L + \epsilon$.


Note: These 2 proofs really do the same thing, but are expressed differently.


\subsection{Some common limits}
Theorem 3.20 is what I find to be one of the hardest in Chapter 3 of PMA.\\
The core ideas are as follows.

The independent variable here is $n$.
\begin{itemize}
	\item It is easier to show the convergence of monomials ($n^p$) than exponentials ($x^n$) because we can take roots($\frac{1}{p}$).
	\item For integer powers (such as in the case of $x^n$), we can use binomial theorem to expand out an exponential expression, e.g. $(1+p)^n$ and extract out a monomial from the binomial expression, so as to prove properties of convergence/divergence.
\end{itemize}

\subsection{Comparison Test}
PMA Theorem 3.25

It is important to note that part (a) of this theorem holds not just for real sequences, but for complex (and $\mathbb{R}^k)$ sequences as well.

See Ex 3.15 for more extensions to $\mathbb{R}^k$.

\subsection{Series of Nonnegative terms}
This will cover PMA 3.26 to 3.29.

\subsubsection{Values of monomial $n^p$}
Assume $n > 1$.
We will prove explicitly that $a,b\in \mathbb{R}, a < 0 < b$ implies $n^a < 1 < n^b$. While this may be trivial for integers, this actually needs to be proven for rationals, then for reals. We will only prove the right inequality, since the left inequality is analogous.\\
Note that $n^0=1$ by definition.

Suppose $0 < b=\frac{p}{q}\in \mathbb{Q}$. Then $n^p \geq n = n^1 > 1$ since $p$ is a positive integer. Hence $n^b=(n^p)^\frac{1}{q}>1$. (Easily proven by contradiction).\\
The result for positive real $b$ then follows by considering supremums.

Given the above result, together with the theorem that $\forall x,y\in \mathbb{R}, n^{x+y}=n^xn^y$, we can then prove a more general result where $a < b\implies n^a < n^b$.\\
As a start, $a < b\implies \exists c > 0, b = a + c$ and $n^c > 1$.


\textbf{Remark}: With this understanding, we can now rest assured when we claim that $2^{(1-p)}<1 \iff (1-p) < 0$ in the proof of theorem 3.28.

\subsubsection{Theorem 3.29}
A supplement to the proof of theorem 3.29. (\textit{I am not sure why Rudin did not write this detail in. Either it is too trivial, or I am missing something here.})

First of all, it is not obvious whether $\frac{1}{n(\log n)^p}$ is monotonically decreasing for $p<0$. Because one of the conditions for applying theorem 3.27 is for the terms of the sequence to be monotonically decreasing. Hence, I would just temporarily assume that $p\geq 0$, and work from there.

So assuming $p\geq 0$, following Rudin's proof, we will arrive at the conclusion that the series converges if $p > 1$ and diverges is $0\leq p \leq 1$. Then, we make the observation that $p < 0$ implies that the terms of the series $\frac{1}{n(\log n)^p} > \frac{1}{n(\log n)^0}=\frac{1}{n}$, and by the comparison test, the series also diverges for $p < 0$.


\subsection{Power Series}
\subsubsection{Theorem 3.39}
The best way to remember this result is by the equality 
\begin{align*}
	\limsup_{n\rightarrow \infty}\sqrt[n]{|c_nz^n|}=|z|\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}
\end{align*}
Notice that this result holds even if $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}=+\infty$, i.e. this result holds for all values of $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}\in [0, +\infty]$. It simply says that the subsequential limits of LHS and RHS are unbounded.

Hence we let $\alpha=\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}$, then $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_nz^n|}<1 \iff |z|\alpha<1$.\\
For finite $\alpha$, divide both sides by $\alpha$ to get $|z|<\frac{1}{\alpha}$.\\
For $\alpha=0$, clearly $\forall z\in \mathbb{C}, |z|\alpha=0<1$, hence $R=\infty$, where $R$ is radius of convergence.\\
For $\alpha=\infty$, then $\forall z\in \mathbb{C}, |z|>0 \implies |z|\alpha=\infty > 1$, hence $R=0$.

By defining $\frac{1}{0}=+\infty$ and $\frac{1}{+\infty}=0$, we then have $R=\frac{1}{\alpha}$ for all values of $\alpha \in [0, +\infty]$.

\subsection{Summation by parts}
This covers theorem 3.41.

The following is what I recall from the book Concrete Mathematics Chapter 2. Some terminology may be different from in the book.

We introduce the following notions of finite calculus.\\
For a discrete function $f: \mathbb{Z} \rightarrow \mathbb{R}$, define the difference operator $(\Delta f)(x) = f(x+1) - f(x)$.\\
Define the summation operator (the discrete analog of the antiderivative) as $\sum \Delta f(x) \delta x=f(x)+C$. i.e. the summation operator the reverse operation to the difference operator. Call this the antidifference. Here, $C$ is a constant, just like antiderivatives, where particular antiderivatives belonging to the same equivalence class/family of antiderivatives differ by a constant. \\
Define the definite summation (the discrete analog of the definite integral) as $\sum_p^q \Delta f(x) \delta x = f(q) - f(p)$.

We make the observation that $\sum_p^q f(x) \delta x = \sum_{p\leq n < q}f(n)$. Note that LHS is the definite summation operator, whereas the RHS is a regular sum.

\subsubsection{Discrete summation by parts}
With these definitions, we can derive some results. Recall that in calculus, for differentiable functions $f,g$, the product rule states $(D(fg))(x) = (Df)(x)g(x) + f(x)(Dg)(x)$ for all $x$ in domain of definition, where $D$ is the derivative operator.

Similarly, there is a product rule for finite calculus.
\begin{align*}
	(\Delta (fg))(x) &= f(x+1)g(x+1) - f(x)g(x)\\
	&= f(x+1)g(x+1) - f(x+1)g(x) + f(x+1)g(x) - f(x)g(x)\\
	&= f(x+1)(g(x+1)-g(x)) + (f(x+1)-f(x))g(x)\\
	&= f(x+1)(\Delta g)(x) + (\Delta f)(x)g(x)
\end{align*}
Hence, applying the summation operator to both sides,
\begin{align*}
	(fg)(x) + C = \sum (f(x+1)(\Delta g)(x) + (\Delta f)(x)g(x)) \delta x\, , C\in \mathbb{R}
\end{align*}

Rearranging then recovers the rule for discrete summation by parts.
\begin{align*}
	\sum (\Delta f)(x)g(x)) \delta x = (fg)(x) - \sum f(x+1)(\Delta g)(x) \delta x
\end{align*}
Again, similar to antiderivatives, the constant $C$ is absorbed by the summation expression.

Notice that here, we split a single antidifference into 2. Why does this work? In regular calculus, this is because $f, g$ are assumed to be continuously differentiable, such that $Df$, $Dg$ are continuous, and thus $Df\cdot g$, $f\cdot Dg$ are continuous, and hence antiderivatives $\int (Df)(x)g(x)\, dx, \int f(x)(Dg)(x)\, dx$ both exist.\\
In finite calculus, the definite summation of a function always exists, since regular summation is always well-defined. And consequently, we can define $F(p) = \sum_0^p f(x)\delta x = \sum_{0\leq x < p}f(x)$. Call this $F$ the indefinite summation of $f$ (\textit{see remark below}). Then $(\Delta F)(x) = F(x+1)-F(x) = f(x)$. This tells us that the indefinite summation is a particular antidifference.
Hence, the antidifference always exists.\\
Notice the contrast between the indefinite integral and the indefinite summation. The indefinite integral is only guaranteed to be a particular antiderivative for continuous functions, whereas indefinite summation is always a particular antidifference for any discrete function defined on $\mathbb{Z}$.


Applying limits of summation, we obtain,
\begin{align*}
	\sum_p^q (\Delta f)(x)g(x)) \delta x = (fg)(x)|_p^q - \sum_p^q f(x+1)(\Delta g)(x) \delta x
\end{align*}

Writing this in terms of a regular sum, we have,
\begin{align*}
	\sum_{p\leq x < q} (\Delta f)(x)g(x)) = (fg)(x)|_p^q - \sum_{p\leq x < q} f(x+1)(\Delta g)(x)
\end{align*}

This lends to a second proof of theorem 3.41.
Define $a_n,b_n,A_n$ as in the book. Also let $A(n) = A_n$. Then $(\Delta A)(n-1) = A_n - A_{n-1} = a_n$.\\
Theorem 3.41 can then be proven by using $p, q+1$ as limits of definite summation.

\textbf{Remark}: Notice that I am quite hesitant to speak of indefinite integrals. This is because, inspired by Prof Chin Chee Whye in MA2104 Multivariable Calculus, indefinite integrals are defined as $F(p)=\int_0^pf(x)dx$ and should not be viewed as the antiderivative for continuous functions $f$. At the very most, indefinite integrals are \textbf{particular} antiderivatives for continuous functions.\\
Hence, when speaking about the reverse process of differentiation, it is better to use terminology like antiderivatives. Similarly, the reverse of difference should be referred to as antidifference. Indefinite summation refers to a specific function, $\sum_0^p f(x)\delta x$.

\subsection{Limits of truncated sums}
\textbf{Proposition}: For all $n\in \mathbb{N}, \sum_{k=1}^na_k+\sum_{k=n+1}^\infty a_k=\sum_{k=1}^\infty a_k$

\textbf{Proof}: Fix $n\in \mathbb{N}$. For all $N\in \mathbb{N}, N>n$, we have $\sum_{k=1}^na_k+\sum_{k=n+1}^N a_k=\sum_{k=1}^N a_k$.\\
Rearranging, we get $\sum_{k=n+1}^N a_k=\sum_{k=1}^N a_k-\sum_{k=1}^na_k$. As $N\rightarrow \infty$, RHS converges to $\sum_{k=1}^\infty a_k-\sum_{k=1}^na_k$. Hence the LHS converges, and $\sum_{k=n+1}^\infty a_k=\sum_{k=1}^\infty a_k-\sum_{k=1}^na_k$.

This leads to the following result. 

Suppose $\sum_{k=1}^\infty a_k=L$. Then fix $\epsilon >0$ and choose $N\in \mathbb{N}$ such that $n\geq N\implies |\sum_{k=1}^{n}a_k - L|<\epsilon$.

By the previous proposition, $|\sum_{k=1}^{n}a_k - L|=|\sum_{k=n+1}^\infty a_k|<\epsilon$.


Alternatively, we can prove this using Cauchy property of convergent sequences, finding $N\in \mathbb{N}$ such that $m>n\geq N\implies |\sum_{k=n}^ma_k|<\frac{\epsilon}{2}$. Letting $m\rightarrow \infty$, we get $|\sum_{k=n}^\infty a_k|\leq \frac{\epsilon}{2} < \epsilon$.


\subsection{Rearrangements}
This will cover PMA 3.52 to 3.55.

\subsubsection{Theorem 3.55}
See the first answer of this \href{https://math.stackexchange.com/questions/523979/theorem-3-55-rudin-rearrangement-and-convergence}{math.stackexchange post} for how we get the $\epsilon$ upper bound for $|s_n-s_n'|$.

The consequence of this bound is that $s_n - \epsilon <s_n'<s_n + \epsilon$, and by taking the limsup and liminf, we get
\begin{align*}
	L - \epsilon \leq \liminf_{n\rightarrow \infty}s_n'\leq \limsup_{n\rightarrow \infty}s_n' \leq L + \epsilon
\end{align*}
and hence $L = \lim_{n\rightarrow \infty}s_n'$. (Here, $L$ is defined to be the limit of $s_n$)

However, note that it is also possible to prove directly that $\lim_{n\rightarrow \infty} s_n' = L$.

\textbf{Alternative Proof}

Before we begin, define the permutation function $\sigma: \mathbb{N}\rightarrow \mathbb{N}$ such that $\forall N\in \mathbb{N}, a_{\sigma(n)}=a_n'$.

Fix $\epsilon > 0$. \\
Choose $N_0\in \mathbb{N}$ such that $n\geq N_0\implies |s_n - L| < \frac{\epsilon}{2}$.\\
Choose $N_1\in \mathbb{N}$ such that $\{1, 2, \dots, N_0\}\subset \{\sigma(1),\dots, \sigma(N_1)\}$\\
Choose $N_2\in \mathbb{N}$ such that $n\geq N_2\implies \sum_{k=n}^\infty |a_k|<\frac{\epsilon}{2}$. This is possible since $\{s_n\}$ converges absolutely.

Let $N=\max(N_1, N_2)$. Then $n\geq N\implies$
\begin{align*}
	|s_n - L| &= |\sum_{k=1}^n a_k' - L|\\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + |\sum_{1\leq k\leq n; \sigma^{-1}(k) > N_1}a_k'| &&\text{by triangle inequality} \\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + \sum_{1\leq k\leq n; \sigma^{-1}(k) > N_1}|a_k'| &&\text{by triangle inequality} \\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + \sum_{k > N_1}|a_k|\\
	&< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align*}
And we are done.

Compared to the book's proof, this involves the choice of $N_0$, which isn't needed in the book.


\subsection{Identities related to $e$}
\begin{itemize}
	\item $(1+\frac{1}{n})^n$ is a strictly increasing sequence
	\item $(1+\frac{1}{n})^{n+1}$ is a strictly decreasing sequence
	\item The tail sum $e-\sum_{0\leq k\leq n}\frac{1}{k!}=\sum_{n<k}\frac{1}{k!}<\frac{1}{n!n}$
\end{itemize}

\textbf{Proof} Use Bernoulli's inequality. The key step in the proofs is as follows.\\
For the first inequality,
\begin{align*}
	\left(1-\frac{1}{(n+1)^2}\right)^{n+1} > \left(1-\frac{n+1}{(n+1)^2}\right)
\end{align*}
For the second inequality,
\begin{align*}
	\left(1+\frac{1}{n(n+2)}\right)^{n+1} > \left(1+\frac{n+1}{n(n+2)}\right) > \left(1-\frac{n+1}{(n+1)^2}\right)
\end{align*}
The key thing to understand here is that since Bernoulli's inequality states that $(1+x)^r\geq 1+rx$, if our working will produce an expression of the form  $(1+x)^r$, we can only go from bigger to smaller. Hence, we should start out with the expression that we wish to prove is larger.\\
For example, to show $x_n=(1+\frac{1}{n})^n$ strictly increasing, we will want to show the expression $\frac{x_{n+1}}{x_n}>1$.\\
To show $y_n=(1+\frac{1}{n})^{n+1}$ strictly decreasing, we will want to show the expression $\frac{y_{n+1}}{y_n}<1$. However, we will want to prove $\frac{y_{n}}{y_{n+1}}>1$ instead, due to the direction of Bernoulli's inequality.


\section{Functions and Continuity}
Check out this \href{https://math.stackexchange.com/questions/455296/can-a-function-with-just-one-point-in-its-domain-be-continuous}{math.stackexchange post} on some of the subtleties of defining limits of a function and continuity. 


\subsection{Uniform continuity}
An alternative to Rudin's proof is presented in class. (this is much easier!)

\textbf{Theorem} Let $f:X\rightarrow Y$ be a continuous mapping. Suppose $X$ is compact. Then $f$ is uniformly continuous on $X$.


Proof outline:
Suppose $f$ is not uniformly continuous. Then $\exists \epsilon, \forall \delta>0, \exists x,y\in X, 0<d(x,y)<\delta \text{ but } d(f(x), f(y)) \geq epsilon$. Letting $\delta_n=\frac{1}{n}$, we then define a sequence $(x_n,y_n)$ such that $\lim_{n\rightarrow \infty}d(x_n,y_n)=0$ but $d(f(x_n), f(y_n)) \geq \epsilon$.

Compactness is equivalent to sequential compactness.\\
We use this to prove 2 things.\\
First, $X$ compact implies that $X^2$ is compact.\\
Next, since $(x_n,y_n)$ is a sequence in $X^2$, we use sequential compactness of $X^2$ to find convergent $(x_{n_i},y_{n_i})$, say it converges to $(x,y)$. Since $0\leq d(x,y)\leq d(x,x_{n_i}) + d(x_{n_i}, y_{n_i}) + d(y_{n_i},y)$, we must have $d(x,y)=0$, hence $x=y$.

Finally, using continuity of $f$, $f(x_{n_i}), f(y_{n_i})\rightarrow f(x)$. And we are have a contradiction since $\lim_{i\rightarrow \infty}x_{n_i}-y_{n_i}=0$. 

Of course, note that the range of the sequences $\{x_{n_i}\}, \{y_{n_i}\}$ may be finite (i.e. we have infinitely many repeating elements). But it doesn't matter, since all we want to show is that there exists $x_{n_i}\neq y_{n_i}$ such that $d(f(x_{n_i}),f(y_{n_i}))\leq \epsilon$.

\subsection{Connectedness}
\textbf{Definition} $X$ is said to be path connected if $\forall x,y\in X$, $\exists \mathbf{r}:[0,1]\rightarrow C$, where $\mathbf{r}$ is continuous, $\mathbf{r}(0)=x,\mathbf{r}(1)=y$ and $C$ is a curve joining $x,y$.

In many well behaved sets, connectedness and path connectedness are either both present or both absent. However, there are exceptions.\\
Consider $S=A\cup B$, where
\begin{align*}
	A &= \{(x,y):y=\sin \frac{1}{x}, x>0\}\\
	B &= \{(0,y):-1\leq y\leq 1\}
\end{align*}
Claim: $S$ is connected, but not path-connected.

\section{Final Exam Cheatsheet}
\paragraph{Countability} Suppose $f:X\rightarrow Y$ is an injection and $Y$ countable. Then $X$ is at most countable. (Note that an injection is equivalent to a bijection from $X$ to $f(X)\subset Y$ via $f$, and $f(X)$ is at most countable.)

	\paragraph{Sets and Functions} Given $f: X\rightarrow Y$. Let $A\subseteq X, B\subseteq Y$.
	\begin{itemize}
		\item $A\subseteq f^{-1}(f(A))$
		\item $f(f^{-1}(B))\subseteq B$
		\item $f^{-1}(B_1 \cup B_2) = f^{-1}(B_1)\cup f^{-1}(B_2)$ 
		\item $f^{-1}(B_1 \cap B_2) = f^{-1}(B_1)\cap f^{-1}(B_2)$ 
		\item $f^{-1}(B^C)=f^{-1}(B)^C$
		\item If $f$ bijective, $f(A^C)=f(A)^C$
	\end{itemize}
	\paragraph{Topology}
	\begin{itemize}
		\item Metric: $d(p,p)=0,\, p\neq q\implies d(p,q)>0,\quad d(p,q)=d(q,p),\quad d(p,q)\leq d(p,r)+d(r,q)$ 
		\item Set $E$ is perfect if $E$ closed and every point of $E$ is a limit point of $E$.
		\item Heine-Borel: $E$ compact $\iff$ Every infinite subset of $E$ has a limit point in $E$.
		\item Every open set in $\mathbb{R}^1$ can be expressed as the union of at most countable disjoint open intervals.
	\end{itemize}
	\paragraph{Properties of Iverson bracket}
	\begin{itemize}
		\item Suppose $A, B$ disjoint. Then $[x\in A] + [x\in B] = [x\in A\cup B]$
		\item Suppose $A\subset B$. Then $[x\in B] - [x\in A] = [x\in B-A]$
	\end{itemize}
	Application: Suppose $f(x)=\sum_n c_n[x_n\leq x]$. Given $y > x$, 
	\begin{align*}
	f(y)-f(x)=\sum_n c_n([x_n\leq y]-[x_n\leq x])=\sum_n c_n([x_n\in (-\infty, y]]-[x_n\in (-\infty, x]])=\sum_n c_n([x_n\in (x, y]]
	\end{align*}
	\paragraph{Properties of closure}
	\begin{itemize}
		\item $\overline{A}$ is the smallest closed set containing $A$. i.e. If $A\subseteq B$, $B$ closed, then $\overline{A}\subseteq B$.
	\end{itemize}
	\paragraph{Intersection of compact sets (Theorem 3.10)} If $K_n$ is a decreasing sequence of compact sets ($K_{n+1}\subseteq K_n$) and if $\lim_{n\rightarrow \infty} diam(K_n) = 0$, then $\cap_1^\infty K_n$ consists of precisely one point.
	\paragraph{Equivalence of Cauchy property and Convergence (3.11)} (1) All compact metric spaces are complete. (2) Euclidean space is complete. (3) Every closed subset of a compact metric space is complete.
	\paragraph{Summation by parts}
		$\sum_{n=p}^qa_nb_n=\sum_{n=p}^{q-1}A_n(b_n-b_{n+1})+A_qb_q-A_{p-1}b_p$
	\paragraph{Characterisation of continuity (Theorem 4.8)} $f: X\rightarrow Y$. Then $f$ continuous on $X$ iff for all open sets $V$ in $Y$, preimage $f^{-1}(V)$ open in $X$.
	\paragraph{Inverse of continuous bijective function (Theorem 4.17)} $f:X\rightarrow Y$, $X$ compact. If $f$ is continuous on $X$ and invertible, then $f^{-1}$ is continuous on $Y$.
	\paragraph{Root Test} Given $\sum a_n$, let $\alpha=\limsup_{n\rightarrow \infty}\sqrt[n]{|a_n|}$.
	\begin{enumerate}
		\item If $\alpha<1$, $\sum a_n$ converges
		\item If $\alpha>1$, $\sum a_n$ diverges
		\item If $\alpha=1$, inconclusive
	\end{enumerate}
	\paragraph{Dirichlet's Test} $\sum_n a_nb_n$ converges if
	\begin{enumerate}
		\item Partial sums $A_n$ bounded
		\item $b_n$ monotonically decreasing
		\item $\lim_{n\rightarrow \infty}b_n = 0$
	\end{enumerate}
	\paragraph{Abel's Test} $\sum_n a_nb_n$ converges if
	\begin{enumerate}
		\item Partial sums $A_n$ converges
		\item $b_n$ monotone and bounded
	\end{enumerate}
	\paragraph*{Uniform Convergence Tests}
	\subparagraph{Equivalence between uniformly Cauchy and uniform convergence} Suppose $f_n$ is a sequence of real functions and $\forall \epsilon>0, \exists N>0, n>m>N\implies ||f_n-f_m||_{\infty}<\epsilon$ (where $||\cdot||_{\infty}$ is the sup norm). Then this is a sufficient and necessary condition for $f_n$ to uniformly converge on $\mathbb{R}$.\\
	This can be easily restated in terms of series $\sum_n f_n(x)$.
	\subparagraph{M-test (Comparison test)} Given $\sum_n f_n(x)$, suppose there exists sequence $(M_n), M_n>0$ such that $\forall x, |f_n(x)|<M_n$. If $\sum_n M_n$ converges, then $\sum_n f_n(x)$ uniformly converges to some $f$.
	\subparagraph{Dirichlet's Test} $\sum_n a_n(x)b_n(x)$ converges uniformly if
	\begin{enumerate}
		\item Partial sums $A_n(x)$ uniformly bounded
		\item $b_n(x)$ monotonically decreasing
		\item $\lim_{n\rightarrow \infty}b_n(x) = 0$ (uniform convergence to 0)
	\end{enumerate}
	\subparagraph{Abel's Test} $\sum_n a_n(x)b_n(x)$ converges if
	\begin{enumerate}
		\item Partial sums $A_n(x)$ converges uniformly
		\item $b_n(x)$ monotone for each $x$ (monotonicity need not be uniform) and uniformly bounded
	\end{enumerate}
	\paragraph{Identities related to $e$}
\begin{itemize}
	\item $(1+\frac{1}{n})^n$ is a strictly increasing sequence; $(1+\frac{1}{n})^{n+1}$ is a strictly decreasing sequence. Proof by Bernoulli's inequality.
	\item Hence, $(1+\frac{1}{n})^n < e < (1+\frac{1}{n})^{n+1}\implies n\ln(1+\frac{1}{n}) < 1 < (n+1)\ln(1+\frac{1}{n})\implies \frac{1}{n+1}<\ln(1+\frac{1}{n})<\frac{1}{n}$
	\item The tail sum $e-\sum_{0\leq k\leq n}\frac{1}{k!}=\sum_{n<k}\frac{1}{k!}<\frac{1}{n!n}$
\end{itemize}
\paragraph{Convex function} Suppose $f$ is a convex function, with $f(\lambda x+(1-\lambda y)\leq \lambda f(x)+(1-\lambda)f(y)$. If $s<t<u$, then 
	$\frac{f(t)-f(s)}{t-s}\leq \frac{f(u)-f(s)}{u-s}\leq \frac{f(u)-f(t)}{u-t}$
The proof is by letting $t=\lambda s+(1-\lambda)u$.
\paragraph{Inequalities}
\begin{itemize}
	\item If $a+b=c+d$ and $0 < a < c\leq d < b$, then $cd>ab$. Proof by considering $cd=\frac{1}{4}((d+c)^2-(d-c)^2)$ and likewise for $ab$.
	\item Bernoulli's inequality: $(1+x)^r\geq 1+rx,\, r\geq 1, x\geq -1$ and $(1+x)^r\leq 1+rx,\, 0\leq r\leq 1, x\geq -1$
	\item When $\forall k, 0< a_k < 1, \prod_k (1-a_k)\geq 1-\sum_k a_k$
	\item Suppose $a_n + b_n\rightarrow c_n+L$, then we can write $a_n + b_n = c_n + L + \epsilon_n$, where $\epsilon_n\rightarrow 0$
	\item If $f$ uniformly continuous on $\mathbb{R}$, then $f(x)=\Theta(x)$.
\end{itemize}
\textbf{Techniques}\\
\textbf{1} Uniform convergence of $x^n(1-x)$ on $[0,1]$. The idea is to split $[0,1]$ into $[0,a],[a,1]$ and show convergence on both intervals.\\
\textbf{2} If a sequence of continuous functions $f_n$ converges uniformly to $f$, then $f$ is also continuous. Usually, the \textbf{converse} of this is used. \textbf{3} Attempt to look for closed form if possible. \textbf{4} For Dirichlet's and Abel's test, $b_n$ does not have to be monotonic from the first term. We just need to show monotonicity for all but finitely many terms.\\
\textbf{Binomial series} $(1+x)^\alpha=\sum_{k=0}^\infty \binom{\alpha}{k}x^k$\\
\textbf{Logarithm series} $\ln(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\dots=\sum_{n=1}^\infty (-1)^{n+1}\frac{x^n}{n}$

\end{document}



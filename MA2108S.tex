\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}

\newcommand\tab[1][1cm]{\hspace*{#1}}

% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\title{MA2108S (Mathematical Analysis I)}
\author{Jia Cheng}
\date{January 2021}

\begin{document}

\maketitle

\section{The real numbers}
\subsection{$p$-th powers and roots are order preserving ($p > 0$)}
It is very easy to see that for $n\in \mathbb{Z}^+$, taking $n$-th roots on $r>0$ preserves order. That is, $0 < r_1 < r_2 \implies r_1^{\frac{1}{n}} < r_2^\frac{1}{n}$. This can be easily proven by contradiction.

However, later in chapter 3, theorem 3.20(a) uses the idea that given $p>0$, $\frac{1}{\epsilon} < n^p \iff (\frac{1}{\epsilon})^\frac{1}{p} < n$. Here, $p$ is a positive real number, not just a positive integer.


PMA Ex 1.6 gives a definition on rational powers, then real powers of positive real numbers.

Ex 1.6 shows that it makes sense to define rational powers. Now, we show that positive rational powers $q > 0$ are order preserving. Since $q>0$, $q=\frac{m}{n}$ for some $m,n\in \mathbb{N}^+$. Suppose $0<r_1<r_2$.\\
Then $r_1^\frac{1}{n} < r_2^\frac{1}{n}$. Hence, $(r_1^\frac{1}{n})^m < (r_2^\frac{1}{n})^m$. By definition of rational powers, $r_1^q < r_2^q$.\\
So now we have proven that taking rational powers is order preserving. This implies that taking rational roots is also order preserving. To define rational roots, we need to prove that $(r^a)^b=r^{ab}$. Then, $(r^\frac{1}{q})^q=r^1=r$ will be a consequence of this.

To show that real powers are also order preserving, I believe it suffices to consider supremums of sets and show that one supremum is larger than the other. (see definition of real powers in Ex 1.6)

I believe we might also need to prove that $(r^\frac{1}{p})^p$ indeed recovers $r$ for real $p$, or more generally, $(r^a)^b=r^{ab}$.

\subsection{Methods of bounding $b^\frac{1}{n}, b>1$}
In both proofs, we assume $M > 1$

\textbf{Method 1} (Ex 1.7)
In general, $x^n - 1 = (x-1)(x^{n-1} + \dots + 1)$\\
Hence, $b - 1 \geq n(b^\frac{1}{n}-1)$

$b^\frac{1}{n}<M \iff b^\frac{1}{n} - 1 < M - 1$\\
It suffices to show $b^\frac{1}{n}-1 \leq \frac{b-1}{n} < M - 1$\\
The strict inequality will hold iff $n > \frac{b-1}{M-1}$.

\textbf{Method 2}
This uses binomial expansion and Bernoulli's inequality.
$b^\frac{1}{n}<M \iff b < M^n = (1 + (M - 1))^n$\\
It suffices to show $b < 1 + n(M - 1) \leq M^n$.\\
The strict inequality will hold iff $n > \frac{b-1}{M-1}$.


\subsection{Complex Numbers and $\mathbb{R}^2$}
Both $\mathbb{C}$ and $\mathbb{R}^2$ are normed spaces. With respect to metrics, these 2 spaces are essentially equivalent. 

The only difference between $\mathbb{C}$ and $\mathbb{R}^2$ is the inner product defined on them (and also the multiplication operation on complex numbers). For $\mathbb{R}^2$ (and $\mathbb{R}^n$ in general), it is the dot/scalar product. For $\mathbb{C}$ (and $\mathbb{C}^n$), it is the Hermitian product. \\
Regardless, the normed induced by these inner products with respect to $\mathbb{C}$ and $\mathbb{R}^2$ are effectively the same.
A complex number can be represented by a 2-tuple $(a,b)$, and it's norm is given by $a^2+b^2$, which the same for a tuple in $\mathbb{R}^2$.

Hence, many theorems that apply to sequences in $\mathbb{R}^2$ later on also apply to complex number sequences.

\subsection{Dense sets}
(Theorem 1.20)\\
The notion of denseness in Chapter 1 claims that, to prove $S$ is dense in $\mathbb{R}$, it is equivalent to show that $\forall p,q\in \mathbb{R}, p<q, \exists x\in S, p<x<q$.\\
Note that in general, this is not always true for any set.

For instance, consider $\mathbb{Z}$. $\mathbb{Z}$ is dense with respect to itself. However, between $0$ and $1$, there exists no element of $\mathbb{Z}$.

Of course with respect to $\mathbb{R}$, topological denseness of a set $S$ is the same as saying that we can always find another element of $S$ between two elements $p,q$ of $\mathbb{R}$.


\subsection{Linear Algebra}
The following theorem can be developed without the notion of basis, linear independence or dimension. 
We simply need definition of orthogonality.
It is useful when dealing with orthogonality.

Given subspace $U$ of Euclidean vector space $V$ and vector $v\in V$.
Also assume that $U$ is spanned by orthogonal vectors $\{u_1,\dots, u_m\}$.
Then there exists vectors $p\in U$, $n\in V$, where $n$ is orthogonal to $U$.\\
In particular, 
\begin{align*}
	p = \sum_{i=1}^m\frac{v\cdot u_i}{|u_i|^2}u_i,\, n=v-p
\end{align*}
We call $p$ the projection of $v$ on $U$.

This is used in ex 1.18.


\subsection{Strategies}
\subsubsection{To show $\sup A\leq \sup B$}
One of the ways to show this is the following:\\
For each $a\in A$, there exists $b\in B, a \leq b$.\\
Take supremum of RHS to get $a\leq \sup B$.\\
Use definition of supremum to get $\sup A \leq \sup B$.

\subsubsection{To show $\inf A\leq \inf B$}
This mirrors the previous section. But we'll show it, for clarity.\\
For each $b\in B$, there exists $a\in A, a\leq b$.\\
Take infimum of LHS to get $\inf A\leq b$.\\
Use definition of infimum to get $\inf A\leq \inf B$.




\section{Point Set Topology}
\subsection{Countability}
\textbf{Proposition} If there is an injection from $A$ to countable set $B$, then $A$ is at most countable.

An alternative way of saying this is that if there is a 1-1 correspondence between $A$ and some subset of $B$, then $A$ is at most countable.

\subsection{Definitions}
\begin{itemize}
	\item Metric Space $M$
	\item Open ball $B(p,r)=\{q\in M : d(p,q) < r \}$
	\item Boundary of S: $bd(S)=\{p\in S : \forall r > 0, \exists q\in S, \exists q'\in S^c, q,q'\in B(p,r) \}$
	\item Limit points of S: $lim(S)=\{p\in S : \forall r > 0, \exists q\in S, q\neq p, q\in B(p,r) \}$
	\item Interior of S: $int(S)=\{p\in S : \exists r > 0, B(p,r)\subseteq S\}$
\end{itemize}
\subsection{Equivalence of definitions of closed set}
The following definitions of a closed set are equivalent.

\begin{enumerate}
	\item $bd(S)\subseteq S$
	\item $lim(S)\subseteq S$
\end{enumerate}

\textbf{Proof}: Suppose $bd(S)\subseteq S$. 
Let $x\in lim(S)$ and fix some arbitrary $r\in \mathbb{R}^+$.
If $x\in S$, we are done.\\
Otherwise,  $x\not \in S$. As $x$ is a limit point, $\exists q\in S, q\neq x$ such that $q\in B(x,r)$. Now, notice that $x$ is a boundary point of $S$, since $x$ itself is not in S, and $q$ is in S, and both $x,q\in B(x,r)$.\\
But by our initial assumption, we have $x\in bd(S)\subseteq S$. Contradiction.\\
Hence, $x\in S$ and $\lim(S)\subseteq S$.

Conversely, suppose $lim(S)\subseteq S$.\\
Let $x\in bd(S)$ and fix some arbitrary $r\in \mathbb{R}^+$. If $x\in S$, we are done.\\
Otherwise, $x\not \in S$. As $x$ is a boundary point, $\exists q\in S, q' \in S^c$ such that both are in $B(x,r)$. In particular, $q\neq x$ since $x\not \in S$. This says that $x$ is a limit point of $S$.\\
Our initial supposition says that $x\in lim(S)\subseteq S$. Again, we have a contradiction.  

\subsection{Sequential Compactness}
The following 2 notions are equivalent.
\begin{enumerate}
	\item Every infinite subset $E$ of a set $X$ has a limit point in $X$.
	\item $X$ is sequentially compact, that is, for every sequence $\{p_n\}$ in $X$, there exists a subsequence converging to some point of $X$.
\end{enumerate}

To prove (2) using (1), consider 2 cases: a sequence with finite range, and a sequence with infinite range (this case is proven with (1)).

\section{Sequences}
\subsection{Infinite subsets vs sequences}
There is a subtle difference between these 2 concepts. Of course, infinite sets are unordered, whereas sequences are ordered. \\
Additionally, it must be noted that sequences can have repeat elements (with different indices). The range of an infinite sequence may very well be finite. An example would be the sequence $\{i^n\}_{n\in \mathbb{N}}$, with range $\{\pm 1, \pm i\}$.

Also, with regard to infinite sets, we speak of limit points, and with regard to sequences, we speak of subsequence limits. Again, there is a difference between limit points and subsequential limits (also known as accumulation points in some texts).

For example, a point can be a subsequential limit without being a limit point. Consider the earlier defined set $\{i^n\}$. Every element in the range $\{\pm 1, \pm i\}$ is a subsequential limit, but since the range is finite, none of these subsequential limits are limit points of the range.

As a consequence, these 2 propositions say different things:
\begin{itemize}
	\item The set of limit points $E'$ of a set $E$ is closed.
	\item The set of subsequential limits of a sequence $\{p_n\}$ is closed.
\end{itemize}


Similarly, supremum of a finite subset of $\mathbb{R}$ is clearly not a limit point of the set.

\subsubsection{Supremum of closed subset of $\mathbb{R}$ belongs to set}
Like before, we need to consider 2 cases, whether the set if infinite or not.

If the set is finite, the supremum of a set is the maximum of the set, and is clearly an element of the set.\\
If the set is infinite, the supremum is then a limit point of the set, and by definition of "closed", belongs to the set.


\subsection{Subsequential limits in $\mathbb{R}$ and $\pm \infty$}
This discussion is inspired by Rudin 3.15 to 3.17.

Notice that subsequential limits of a real sequence lie in $(-\infty, +\infty)$, i.e. they are finite. $\pm \infty$ are \textbf{not} considered limits.

This is because the definition of a limit $p$ of a sequence $\{p_n\}$ (or a subsequence) involved the notion of distance $d(p_n, p)$ approaching 0. In particular, $d(p_n, p)$ is defined.\\
Clearly, $d(p_n, +\infty)$ or $d(p_n, -\infty)$ are not defined, since metric $d$ is always a real-valued function.

\subsection{The Extended Real Line}
While this was introduced in Chapter 1, it is in Chapter 3 that we see the application of $\mathbb{R} \cup \{\pm \infty\} = [-\infty, +\infty]$.

Properties of the extended reals
\begin{itemize}
	\item The extended real line is no longer a field.
	\item $\mathbb{R}$ has the completeness property, such that for any upper bounded subset $S$ of $\mathbb{R}$, $\sup S$ exists in $\mathbb{R}$. However, $\sup S$ does not exist in $\mathbb{R}$ if $S$ is unbounded.\\
	 In contrast, all subsets $S$ of $[-\infty, +\infty]$ have supremums and infimums. For an upper unbounded set $S$, we would have $\sup S = + \infty$, and for a lower unbounded set $S$, we would have $\inf S = -\infty$. For a bounded set, we can then apply the completeness property of $\mathbb{R}$ to find a supremum/infimum in $\mathbb{R}\subset [-\infty, +\infty]$
\end{itemize}

The fact that all subsets of $[-\infty, +\infty]$ have well-defined supremum and infimum makes Theorem 3.17 in PMA very easy to state. Given the set $E$ of all subsequential limits of a sequence $\{p_n\}$ \textbf{which \textit{additionally} includes $\pm \infty$ if the sequence is unbounded}, we are guaranteed to find $\sup E, \inf E$, without having to talk about special cases (i.e. bounded vs unbounded sequences), which can be troublesome to write.

Of course, if $\sup E$ turns out to be $+\infty$ or $-\infty$, we must be careful. We can't treat the infinities like subsequential limits. For e.g., we can't use a theorem like \textit{the set of subsequential limits is closed}, since $\pm \infty$ do not belong in the set of subsequential limits.\\
To emphasise: $E$ in theorem 3.17 is the union of subsequential limits (which are finite) and possibly $+\infty$ if the sequence is unbounded above, $-\infty$ if the sequence is unbounded below.


\subsection{Two proofs of the Squeeze Theorem}
We are given functions $f,g,h$ where $f(x)\leq g(x)\leq h(x)$ in some neighborhood of $x_0$, such that $f(x)\rightarrow L$ and $h(x)\rightarrow L$. Fix $\epsilon > 0$ and we can obtain the following bound for all $0 < |x - x_0| < \delta$ for some $\delta > 0$.
\begin{align*}
	L - \epsilon < f(x) \leq g(x) \leq h(x) < L + \epsilon
\end{align*}
This says that $g(x)\rightarrow L$ as well.

This can easily be adapted to the proof of squeeze theorem for sequences as well.

A second proof of the squeeze theorem that is more in line with Rudin Chapter 3:

Given sequences $\{a_n\}, \{b_n\}, \{c_n\}$ such that $a_n\leq b_n\leq c_n$ for all but finitely many $n$, and $a_n, c_n\rightarrow L$.

Then we have \begin{align*}
	L = \lim a_n = \liminf a_n \leq \liminf b_n \leq \limsup b_n \leq \limsup c_n = \lim c_n = L
\end{align*}
which implies
\begin{align*}
	\liminf b_n = \limsup b_n = L \implies \lim b_n = L
\end{align*}
The last implies is due to the following: For any $\epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. $n\geq N\implies L - \epsilon = \liminf b_n - \epsilon < b_n < \limsup b_n + \epsilon = L + \epsilon$.


Note: These 2 proofs really do the same thing, but are expressed differently.


\subsection{Some common limits}
Theorem 3.20 is what I find to be one of the hardest in Chapter 3 of PMA.\\
The core ideas are as follows.

The independent variable here is $n$.
\begin{itemize}
	\item It is easier to show the convergence of monomials ($n^p$) than exponentials ($x^n$) because we can take roots($\frac{1}{p}$).
	\item For integer powers (such as in the case of $x^n$), we can use binomial theorem to expand out an exponential expression, e.g. $(1+p)^n$ and extract out a monomial from the binomial expression, so as to prove properties of convergence/divergence.
\end{itemize}

\subsection{Comparison Test}
PMA Theorem 3.25

It is important to note that part (a) of this theorem holds not just for real sequences, but for complex (and $\mathbb{R}^k)$ sequences as well.

See Ex 3.15 for more extensions to $\mathbb{R}^k$.

\subsection{Series of Nonnegative terms}
This will cover PMA 3.26 to 3.29.

\subsubsection{Values of monomial $n^p$}
Assume $n > 1$.
We will prove explicitly that $a,b\in \mathbb{R}, a < 0 < b$ implies $n^a < 1 < n^b$. While this may be trivial for integers, this actually needs to be proven for rationals, then for reals. We will only prove the right inequality, since the left inequality is analogous.\\
Note that $n^0=1$ by definition.

Suppose $0 < b=\frac{p}{q}\in \mathbb{Q}$. Then $n^p \geq n = n^1 > 1$ since $p$ is a positive integer. Hence $n^b=(n^p)^\frac{1}{q}>1$. (Easily proven by contradiction).\\
The result for positive real $b$ then follows by considering supremums.

Given the above result, together with the theorem that $\forall x,y\in \mathbb{R}, n^{x+y}=n^xn^y$, we can then prove a more general result where $a < b\implies n^a < n^b$.\\
As a start, $a < b\implies \exists c > 0, b = a + c$ and $n^c > 1$.


\textbf{Remark}: With this understanding, we can now rest assured when we claim that $2^{(1-p)}<1 \iff (1-p) < 0$ in the proof of theorem 3.28.

\subsubsection{Theorem 3.29}
A supplement to the proof of theorem 3.29. (\textit{I am not sure why Rudin did not write this detail in. Either it is too trivial, or I am missing something here.})

First of all, it is not obvious whether $\frac{1}{n(\log n)^p}$ is monotonically decreasing for $p<0$. Because one of the conditions for applying theorem 3.27 is for the terms of the sequence to be monotonically decreasing. Hence, I would just temporarily assume that $p\geq 0$, and work from there.

So assuming $p\geq 0$, following Rudin's proof, we will arrive at the conclusion that the series converges if $p > 1$ and diverges is $0\leq p \leq 1$. Then, we make the observation that $p < 0$ implies that the terms of the series $\frac{1}{n(\log n)^p} > \frac{1}{n(\log n)^0}=\frac{1}{n}$, and by the comparison test, the series also diverges for $p < 0$.


\subsection{Power Series}
\subsubsection{Theorem 3.39}
The best way to remember this result is by the equality 
\begin{align*}
	\limsup_{n\rightarrow \infty}\sqrt[n]{|c_nz^n|}=|z|\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}
\end{align*}
Notice that this result holds even if $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}=+\infty$, i.e. this result holds for all values of $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}\in [0, +\infty]$. It simply says that the subsequential limits of LHS and RHS are unbounded.

Hence we let $\alpha=\limsup_{n\rightarrow \infty}\sqrt[n]{|c_n|}$, then $\limsup_{n\rightarrow \infty}\sqrt[n]{|c_nz^n|}<1 \iff |z|\alpha<1$.\\
For finite $\alpha$, divide both sides by $\alpha$ to get $|z|<\frac{1}{\alpha}$.\\
For $\alpha=0$, clearly $\forall z\in \mathbb{C}, |z|\alpha=0<1$, hence $R=\infty$, where $R$ is radius of convergence.\\
For $\alpha=\infty$, then $\forall z\in \mathbb{C}, |z|>0 \implies |z|\alpha=\infty > 1$, hence $R=0$.

By defining $\frac{1}{0}=+\infty$ and $\frac{1}{+\infty}=0$, we then have $R=\frac{1}{\alpha}$ for all values of $\alpha \in [0, +\infty]$.

\subsection{Summation by parts}
This covers theorem 3.41.

The following is what I recall from the book Concrete Mathematics Chapter 2. Some terminology may be different from in the book.

We introduce the following notions of finite calculus.\\
For a discrete function $f: \mathbb{Z} \rightarrow \mathbb{R}$, define the difference operator $(\Delta f)(x) = f(x+1) - f(x)$.\\
Define the summation operator (the discrete analog of the antiderivative) as $\sum \Delta f(x) \delta x=f(x)+C$. i.e. the summation operator the reverse operation to the difference operator. Call this the antidifference. Here, $C$ is a constant, just like antiderivatives, where particular antiderivatives belonging to the same equivalence class/family of antiderivatives differ by a constant. \\
Define the definite summation (the discrete analog of the definite integral) as $\sum_p^q \Delta f(x) \delta x = f(q) - f(p)$.

We make the observation that $\sum_p^q f(x) \delta x = \sum_{p\leq n < q}f(n)$. Note that LHS is the definite summation operator, whereas the RHS is a regular sum.

\subsubsection{Discrete summation by parts}
With these definitions, we can derive some results. Recall that in calculus, for differentiable functions $f,g$, the product rule states $(D(fg))(x) = (Df)(x)g(x) + f(x)(Dg)(x)$ for all $x$ in domain of definition, where $D$ is the derivative operator.

Similarly, there is a product rule for finite calculus.
\begin{align*}
	(\Delta (fg))(x) &= f(x+1)g(x+1) - f(x)g(x)\\
	&= f(x+1)g(x+1) - f(x+1)g(x) + f(x+1)g(x) - f(x)g(x)\\
	&= f(x+1)(g(x+1)-g(x)) + (f(x+1)-f(x))g(x)\\
	&= f(x+1)(\Delta g)(x) + (\Delta f)(x)g(x)
\end{align*}
Hence, applying the summation operator to both sides,
\begin{align*}
	(fg)(x) + C = \sum (f(x+1)(\Delta g)(x) + (\Delta f)(x)g(x)) \delta x\, , C\in \mathbb{R}
\end{align*}

Rearranging then recovers the rule for discrete summation by parts.
\begin{align*}
	\sum (\Delta f)(x)g(x)) \delta x = (fg)(x) - \sum f(x+1)(\Delta g)(x) \delta x
\end{align*}
Again, similar to antiderivatives, the constant $C$ is absorbed by the summation expression.

Notice that here, we split a single antidifference into 2. Why does this work? In regular calculus, this is because $f, g$ are assumed to be continuously differentiable, such that $Df$, $Dg$ are continuous, and thus $Df\cdot g$, $f\cdot Dg$ are continuous, and hence antiderivatives $\int (Df)(x)g(x)\, dx, \int f(x)(Dg)(x)\, dx$ both exist.\\
In finite calculus, the definite summation of a function always exists, since regular summation is always well-defined. And consequently, we can define $F(p) = \sum_0^p f(x)\delta x = \sum_{0\leq x < p}f(x)$. Call this $F$ the indefinite summation of $f$ (\textit{see remark below}). Then $(\Delta F)(x) = F(x+1)-F(x) = f(x)$. This tells us that the indefinite summation is a particular antidifference.
Hence, the antidifference always exists.\\
Notice the contrast between the indefinite integral and the indefinite summation. The indefinite integral is only guaranteed to be a particular antiderivative for continuous functions, whereas indefinite summation is always a particular antidifference for any discrete function defined on $\mathbb{Z}$.


Applying limits of summation, we obtain,
\begin{align*}
	\sum_p^q (\Delta f)(x)g(x)) \delta x = (fg)(x)|_p^q - \sum_p^q f(x+1)(\Delta g)(x) \delta x
\end{align*}

Writing this in terms of a regular sum, we have,
\begin{align*}
	\sum_{p\leq x < q} (\Delta f)(x)g(x)) = (fg)(x)|_p^q - \sum_{p\leq x < q} f(x+1)(\Delta g)(x)
\end{align*}

This lends to a second proof of theorem 3.41.
Define $a_n,b_n,A_n$ as in the book. Also let $A(n) = A_n$. Then $(\Delta A)(n-1) = A_n - A_{n-1} = a_n$.\\
Theorem 3.41 can then be proven by using $p, q+1$ as limits of definite summation.

\textbf{Remark}: Notice that I am quite hesitant to speak of indefinite integrals. This is because, inspired by Prof Chin Chee Whye in MA2104 Multivariable Calculus, indefinite integrals are defined as $F(p)=\int_0^pf(x)dx$ and should not be viewed as the antiderivative for continuous functions $f$. At the very most, indefinite integrals are \textbf{particular} antiderivatives for continuous functions.\\
Hence, when speaking about the reverse process of differentiation, it is better to use terminology like antiderivatives. Similarly, the reverse of difference should be referred to as antidifference. Indefinite summation refers to a specific function, $\sum_0^p f(x)\delta x$.

\subsection{Limits of truncated sums}
\textbf{Proposition}: For all $n\in \mathbb{N}, \sum_{k=1}^na_k+\sum_{k=n+1}^\infty a_k=\sum_{k=1}^\infty a_k$

\textbf{Proof}: Fix $n\in \mathbb{N}$. For all $N\in \mathbb{N}, N>n$, we have $\sum_{k=1}^na_k+\sum_{k=n+1}^N a_k=\sum_{k=1}^N a_k$.\\
Rearranging, we get $\sum_{k=n+1}^N a_k=\sum_{k=1}^N a_k-\sum_{k=1}^na_k$. As $N\rightarrow \infty$, RHS converges to $\sum_{k=1}^\infty a_k-\sum_{k=1}^na_k$. Hence the LHS converges, and $\sum_{k=n+1}^\infty a_k=\sum_{k=1}^\infty a_k-\sum_{k=1}^na_k$.

This leads to the following result. 

Suppose $\sum_{k=1}^\infty a_k=L$. Then fix $\epsilon >0$ and choose $N\in \mathbb{N}$ such that $n\geq N\implies |\sum_{k=1}^{n}a_k - L|<\epsilon$.

By the previous proposition, $|\sum_{k=1}^{n}a_k - L|=|\sum_{k=n+1}^\infty a_k|<\epsilon$.


Alternatively, we can prove this using Cauchy property of convergent sequences, finding $N\in \mathbb{N}$ such that $m>n\geq N\implies |\sum_{k=n}^ma_k|<\frac{\epsilon}{2}$. Letting $m\rightarrow \infty$, we get $|\sum_{k=n}^\infty a_k|\leq \frac{\epsilon}{2} < \epsilon$.


\subsection{Rearrangements}
This will cover PMA 3.52 to 3.55.

\subsubsection{Theorem 3.55}
See the first answer of this \href{https://math.stackexchange.com/questions/523979/theorem-3-55-rudin-rearrangement-and-convergence}{math.stackexchange post} for how we get the $\epsilon$ upper bound for $|s_n-s_n'|$.

The consequence of this bound is that $s_n - \epsilon <s_n'<s_n + \epsilon$, and by taking the limsup and liminf, we get
\begin{align*}
	L - \epsilon \leq \liminf_{n\rightarrow \infty}s_n'\leq \limsup_{n\rightarrow \infty}s_n' \leq L + \epsilon
\end{align*}
and hence $L = \lim_{n\rightarrow \infty}s_n'$. (Here, $L$ is defined to be the limit of $s_n$)

However, note that it is also possible to prove directly that $\lim_{n\rightarrow \infty} s_n' = L$.

\textbf{Alternative Proof}

Before we begin, define the permutation function $\sigma: \mathbb{N}\rightarrow \mathbb{N}$ such that $\forall N\in \mathbb{N}, a_{\sigma(n)}=a_n'$.

Fix $\epsilon > 0$. \\
Choose $N_0\in \mathbb{N}$ such that $n\geq N_0\implies |s_n - L| < \frac{\epsilon}{2}$.\\
Choose $N_1\in \mathbb{N}$ such that $\{1, 2, \dots, N_0\}\subset \{\sigma(1),\dots, \sigma(N_1)\}$\\
Choose $N_2\in \mathbb{N}$ such that $n\geq N_2\implies \sum_{k=n}^\infty |a_k|<\frac{\epsilon}{2}$. This is possible since $\{s_n\}$ converges absolutely.

Let $N=\max(N_1, N_2)$. Then $n\geq N\implies$
\begin{align*}
	|s_n - L| &= |\sum_{k=1}^n a_k' - L|\\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + |\sum_{1\leq k\leq n; \sigma^{-1}(k) > N_1}a_k'| &&\text{by triangle inequality} \\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + \sum_{1\leq k\leq n; \sigma^{-1}(k) > N_1}|a_k'| &&\text{by triangle inequality} \\
	&\leq |\sum_{k=1}^{N_1} a_k - L| + \sum_{k > N_1}|a_k|\\
	&< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align*}
And we are done.

Compared to the book's proof, this involves the choice of $N_0$, which isn't needed in the book.


\section{Functions and Continuity}
Check out this \href{https://math.stackexchange.com/questions/455296/can-a-function-with-just-one-point-in-its-domain-be-continuous}{math.stackexchange post} on some of the subtleties of defining limits of a function and continuity. 

\end{document}



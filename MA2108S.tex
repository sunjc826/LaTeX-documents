\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}

\newcommand\tab[1][1cm]{\hspace*{#1}}

% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}


\title{MA2108S (Mathematical Analysis I) Pointers}
\author{Jia Cheng}
\date{January 2021}

\begin{document}

\maketitle

\section{The real numbers}
\subsection{$p$-th powers and roots are order preserving ($p > 0$)}
It is very easy to see that for $n\in \mathbb{Z}^+$, taking $n$-th roots on $r>0$ preserves order. That is, $0 < r_1 < r_2 \implies r_1^{\frac{1}{n}} < r_2^\frac{1}{n}$. This can be easily proven by contradiction.

However, later in chapter 3, theorem 3.20(a) uses the idea that given $p>0$, $\frac{1}{\epsilon} < n^p \iff (\frac{1}{\epsilon})^\frac{1}{p} < n$. Here, $p$ is a positive real number, not just a positive integer.


PMA Ex 1.6 gives a definition on rational powers, then real powers of positive real numbers. (I'm not quite sure why $b>1$ is used in this exercise instead of $b>0$)

Ex 1.6 shows that it makes sense to define rational powers. Now, we show that positive rational powers $q > 0$ are order preserving. Since $q>0$, $q=\frac{m}{n}$ for some $m,n\in \mathbb{N}^+$. Suppose $0<r_1<r_2$.\\
Then $r_1^\frac{1}{n} < r_2^\frac{1}{n}$. Hence, $(r_1^\frac{1}{n})^m < (r_2^\frac{1}{n})^m$. By definition of rational powers, $r_1^q < r_2^q$.\\
So now we have proven that taking rational powers is order preserving. This implies that taking rational roots is also order preserving. To define rational roots, we need to prove that $(r^a)^b=r^{ab}$. Then, $(r^\frac{1}{q})^q=r^1=r$ will be a consequence of this.

To show that real powers are also order preserving, I believe it suffices to consider supremums of sets and show that one supremum is larger than the other. (see definition of real powers in Ex 1.6)

I believe we might also need to prove that $(r^\frac{1}{p})^p$ indeed recovers $r$ for real $p$, or more generally, $(r^a)^b=r^{ab}$.

\section{Point Set Topology}
\subsection{Definitions}
\begin{itemize}
	\item Metric Space $M$
	\item Open ball $B(p,r)=\{q\in M : d(p,q) < r \}$
	\item Boundary of S: $bd(S)=\{p\in S : \forall r > 0, \exists q\in S, \exists q'\in S^c, q,q'\in B(p,r) \}$
	\item Limit points of S: $lim(S)=\{p\in S : \forall r > 0, \exists q\in S, q\neq p, q\in B(p,r) \}$
	\item Interior of S: $int(S)=\{p\in S : \exists r > 0, B(p,r)\subseteq S\}$
\end{itemize}
\subsection{Equivalence of definitions of closed set}
The following definitions of a closed set are equivalent.

\begin{enumerate}
	\item $bd(S)\subseteq S$
	\item $lim(S)\subseteq S$
\end{enumerate}

\textbf{Proof}: Suppose $bd(S)\subseteq S$. 
Let $x\in lim(S)$ and fix some arbitrary $r\in \mathbb{R}^+$.
If $x\in S$, we are done.\\
Otherwise,  $x\not \in S$. As $x$ is a limit point, $\exists q\in S, q\neq x$ such that $q\in B(x,r)$. Now, notice that $x$ is a boundary point of $S$, since $x$ itself is not in S, and $q$ is in S, and both $x,q\in B(x,r)$.\\
But by our initial assumption, we have $x\in bd(S)\subseteq S$. Contradiction.\\
Hence, $x\in S$ and $\lim(S)\subseteq S$.

Conversely, suppose $lim(S)\subseteq S$.\\
Let $x\in bd(S)$ and fix some arbitrary $r\in \mathbb{R}^+$. If $x\in S$, we are done.\\
Otherwise, $x\not \in S$. As $x$ is a boundary point, $\exists q\in S, q' \in S^c$ such that both are in $B(x,r)$. In particular, $q\neq x$ since $x\not \in S$. This says that $x$ is a limit point of $S$.\\
Our initial supposition says that $x\in lim(S)\subseteq S$. Again, we have a contradiction.  

\subsection{Sequential Compactness}
The following 2 notions are equivalent.
\begin{enumerate}
	\item Every infinite subset $E$ of a set $X$ has a limit point in $X$.
	\item $X$ is sequentially compact, that is, for every sequence $\{p_n\}$ in $X$, there exists a subsequence converging to some point of $X$.
\end{enumerate}

To prove (2) using (1), consider 2 cases: a sequence with finite range, and a sequence with infinite range (this case is proven with (1)).

\section{Sequences}
\subsection{Infinite subsets vs sequences}
There is a subtle difference between these 2 concepts. Of course, infinite sets are unordered, whereas sequences are ordered. \\
Additionally, it must be noted that sequences can have repeat elements (with different indices). The range of an infinite sequence may very well be finite. An example would be the sequence $\{i^n\}_{n\in \mathbb{N}}$, with range $\{\pm 1, \pm i\}$.

Also, with regard to infinite sets, we speak of limit points, and with regard to sequences, we speak of subsequence limits. Again, there is a difference between limit points and subsequential limits.

For example, a point can be a subsequential limit without being a limit point. Consider the earlier defined set $\{i^n\}$. Every element in the range $\{\pm 1, \pm i\}$ is a subsequential limit, but since the range is finite, none of these subsequential limits are limit points of the range.

As a consequence, these 2 propositions say different things:
\begin{itemize}
	\item The set of limit points $E'$ of a set $E$ is closed.
	\item The set of subsequential limits of a sequence $\{p_n\}$ is closed.
\end{itemize}


Similarly, supremum of a finite subset of $\mathbb{R}$ is clearly not a limit point of the set.

\subsubsection{Supremum of closed subset of $\mathbb{R}$ belongs to set}
Like before, we need to consider 2 cases, whether the set if infinite or not.

If the set is finite, the supremum of a set is the maximum of the set, and is clearly an element of the set.\\
If the set is infinite, the supremum is then a limit point of the set, and by definition of "closed", belongs to the set.


\subsection{Subsequential limits in $\mathbb{R}$ and $\pm \infty$}
This discussion is inspired by Rudin 3.15 to 3.17.

Notice that subsequential limits of a real sequence lie in $(-\infty, +\infty)$, i.e. they are finite. $\pm \infty$ are \textbf{not} considered limits.

This is because the definition of a limit $p$ of a sequence $\{p_n\}$ (or a subsequence) involved the notion of distance $d(p_n, p)$ approaching 0. In particular, $d(p_n, p)$ is defined.\\
Clearly, $d(p_n, +\infty)$ or $d(p_n, -\infty)$ are not defined, since metric $d$ is always a real-valued function.

\subsection{The Extended Real Line}
While this was introduced in Chapter 1, it is in Chapter 3 that we see the application of $\mathbb{R} \cup \{\pm \infty\} = [-\infty, +\infty]$.

Properties of the extended reals
\begin{itemize}
	\item The extended real line is no longer a field.
	\item $\mathbb{R}$ has the completeness property, such that for any upper bounded subset $S$ of $\mathbb{R}$, $\sup S$ exists in $\mathbb{R}$. However, $\sup S$ does not exist in $\mathbb{R}$ if $S$ is unbounded.\\
	 In contrast, all subsets $S$ of $[-\infty, +\infty]$ have supremums and infimums. For an upper unbounded set $S$, we would have $\sup S = + \infty$, and for a lower unbounded set $S$, we would have $\inf S = -\infty$. For a bounded set, we can then apply the completeness property of $\mathbb{R}$ to find a supremum/infimum in $\mathbb{R}\subset [-\infty, +\infty]$
\end{itemize}

The fact that all subsets of $[-\infty, +\infty]$ have well-defined supremum and infimum makes Theorem 3.17 in PMA very easy to state. Given the set $E$ of all subsequential limits of a sequence $\{p_n\}$ \textbf{which \textit{additionally} includes $\pm \infty$ if the sequence is unbounded}, we are guaranteed to find $\sup E, \inf E$, without having to talk about special cases (i.e. bounded vs unbounded sequences), which can be troublesome to write.

Of course, if $\sup E$ turns out to be $+\infty$ or $-\infty$, we must be careful. We can't treat the infinities like subsequential limits. For e.g., we can't use a theorem like \textit{the set of subsequential limits is closed}, since $\pm \infty$ do not belong in the set of subsequential limits.\\
To emphasise: $E$ in theorem 3.17 is the union of subsequential limits (which are finite) and possibly $+\infty$ if the sequence is unbounded above, $-\infty$ if the sequence is unbounded below.


\subsection{Two proofs of the Squeeze Theorem}
We are given functions $f,g,h$ where $f(x)\leq g(x)\leq h(x)$ in some neighborhood of $x_0$, such that $f(x)\rightarrow L$ and $h(x)\rightarrow L$. Fix $\epsilon > 0$ and we can obtain the following bound for all $0 < |x - x_0| < \delta$ for some $\delta > 0$.
\begin{align*}
	L - \epsilon < f(x) \leq g(x) \leq h(x) < L + \epsilon
\end{align*}
This says that $g(x)\rightarrow L$ as well.

This can easily be adapted to the proof of squeeze theorem for sequences as well.

A second proof of the squeeze theorem that is more in line with Rudin Chapter 3:

Given sequences $\{a_n\}, \{b_n\}, \{c_n\}$ such that $a_n\leq b_n\leq c_n$ for all but finitely many $n$, and $a_n, c_n\rightarrow L$.

Then we have \begin{align*}
	L = \lim a_n = \liminf a_n \leq \liminf b_n \leq \limsup b_n \leq \limsup c_n = \lim c_n = L
\end{align*}
which implies
\begin{align*}
	\liminf b_n = \limsup b_n = L \implies \lim b_n = L
\end{align*}
The last implies is due to the following: For any $\epsilon > 0$, there exists $N \in \mathbb{N}$ s.t. $n\geq N\implies L - \epsilon = \liminf b_n - \epsilon < b_n < \limsup b_n + \epsilon = L + \epsilon$.


Note: These 2 proofs really do the same thing, but are expressed differently.


\subsection{Some common limits}
Theorem 3.20 is what I find to be one of the hardest in Chapter 3 of PMA.\\
The core ideas are as follows.

The independent variable here is $n$.
\begin{itemize}
	\item It is easier to show the convergence of monomials ($n^p$) than exponentials ($x^n$) because we can take roots($\frac{1}{p}$).
	\item For integer powers (such as in the case of $x^n$), we can use binomial theorem to expand out an exponential expression, e.g. $(1+p)^n$ and extract out a monomial from the binomial expression, so as to prove properties of convergence/divergence.
\end{itemize}

\subsection{Series of Nonnegative terms}
This will cover PMA 3.26 to 3.29.

\subsubsection{Values of monomial $n^p$}
Assume $n > 1$.
We will prove explicitly that $a,b\in \mathbb{R}, a < 0 < b$ implies $n^a < 1 < n^b$. While this may be trivial for integers, this actually needs to be proven for rationals, then for reals. We will only prove the right inequality, since the left inequality is analogous.\\
Note that $n^0=1$ by definition.

Suppose $0 < b=\frac{p}{q}\in \mathbb{Q}$. Then $n^p \geq n = n^1 > 1$ since $p$ is a positive integer. Hence $n^b=(n^p)^\frac{1}{q}>1$. (Easily proven by contradiction).\\
The result for positive real $b$ then follows by considering supremums.

Given the above result, together with the theorem that $\forall x,y\in \mathbb{R}, n^{x+y}=n^xn^y$, we can then prove a more general result where $a < b\implies n^a < n^b$.\\
As a start, $a < b\implies \exists c > 0, b = a + c$ and $n^c > 1$.


\textbf{Remark}: With this understanding, we can now rest assured when we claim that $2^{(1-p)}<1 \iff (1-p) < 0$ in the proof of theorem 3.28.

\subsubsection{Theorem 3.29}
A supplement to the proof of theorem 3.29. (\textit{I am not sure why Rudin did not write this detail in. Either it is too trivial, or I am missing something here.})

First of all, it is not obvious whether $\frac{1}{n(\log n)^p}$ is monotonically decreasing for $p<0$. Because one of the conditions for applying theorem 3.27 is for the terms of the sequence to be monotonically decreasing. Hence, I would just temporarily assume that $p\geq 0$, and work from there.

So assuming $p\geq 0$, following Rudin's proof, we will arrive at the conclusion that the series converges if $p > 1$ and diverges is $0\leq p \leq 1$. Then, we make the observation that $p < 0$ implies that the terms of the series $\frac{1}{n(\log n)^p} > \frac{1}{n(\log n)^0}=\frac{1}{n}$, and by the comparison test, the series also diverges for $p < 0$.



\end{document}



\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{gensymb}
\usepackage{listings}
\usepackage{caption}
\usepackage{pgfplots}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\set}[1]{\{#1\}} %braces%
\newcommand{\rbracket}[1]{\left(#1\right)} %round brackets%
\newcommand{\sbracket}[1]{\left[#1\right]} %square brackets%
\newcommand{\floor}[1]{\lfloor #1\rfloor}
\newcommand{\ceil}[1]{\lceil #1\rceil}
\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\reals}[0]{\mathbb{R}} %real numbers%
\newcommand{\real}[0]{\mathbb{R}} %real numbers%
\newcommand{\rational}[0]{\mathbb{Q}} %rational numbers%
\newcommand{\integer}[0]{\mathbb{Z}} %integers%
\newcommand{\nat}[0]{\mathbb{N}} %natural numbers%
\newcommand{\limitinf}[1][n]{\lim_{#1\rightarrow \infty}} %limit to infinity%
\newcommand{\forallReal}[1][x]{\forall #1\in\real} 
\newcommand{\forallInteger}[1][n]{\forall #1\in\integer}
\newcommand{\forallNat}[1][n]{\forall #1\in\nat}
\newcommand{\existsReal}[1][x]{\exists #1\in\real}
\newcommand{\existsInteger}[1][n]{\exists #1\in\integer}
\newcommand{\existsNat}[1][n]{\exists #1\in\nat}
\newcommand{\enum}[1]{1,2,\dots,#1}
\newcommand{\seq}[2]{#1_1,#1_2,\dots,#1_{#2}} %finite sequence literal%
\newcommand{\infseq}[1]{#1_1,#1_2,\dots} %infinite sequence%
\newcommand{\subseq}[3]{#1_{#2_1},#1_{#2_2},\dots,#1_{#2_{#3}}} %finite subsequence literal%
\newcommand{\seqconn}[3]{#1_1 #3 #1_2 #3 \dots #3 #1_{#2}} %sequence delimited by argument 3%
\newcommand{\subseqconn}[4]{#1_{#2_1} #4 #1_{#2_2} #4 \dots #4 #1_{#2_{#3}}} %subsequence literal%

\newcommand{\io}[1]{#1 \mathit{i.o.}} %infinitely often%
\newcommand{\alma}[1]{#1 \mathit{a.a.}} %almost always%

\newcommand{\qedsymbol}{\hfill\blacksquare} %right aligned QED%

% \rule{length}{thickness}
\newcommand{\divider}[0]{\begin{center}
\rule{16cm}{0.5pt}
\end{center}}


% Margins
% top=2.54cm, left=2.54cm, right=2.54cm, bottom=2.54cm
\usepackage[a4paper, top=2.54cm, left=2.54cm, right=2.54cm, bottom=2.54cm]{geometry}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\title{MA3238 (Stochastic Processes 1)}
\author{Jia Cheng}
\date{Jan 2022}

\begin{document}
\maketitle

Reference: Introduction to Probability Models (9th edition)

\section{Revision}

\emph{Limit of the binomial distribution is the poisson distribution} Let $X_n\sim (n, p=\frac{\lambda}{n})$ and consider the distribution of $X_n$ as $n\rightarrow \infty$. \textbf{Note} that as we increase $n$, we also shrink $p$ since that $p=\frac{\lambda}{n}$.

Observe that \begin{align*}
	\binom{n}{k}p^k(1-p)^{n-k} &= \frac{n^{\underline{k}}}{k!} \rbracket{\frac{\lambda}{n}}^k \rbracket{1-\frac{\lambda}{n}}^{n-k}\\
	&= \frac{\lambda^k}{k!}\cdot \frac{n^{\underline{k}}}{n^k}\rbracket{1-\frac{\lambda}{n}}^{n-k}
\end{align*}
Since $\frac{\lambda^k}{k!}$ is independent of $n$, it suffices to take the limit of the latter expression. We have
\begin{align*}
	\limitinf \frac{n^{\underline{k}}}{n^k}\rbracket{1-\frac{\lambda}{n}}^{n-k} &= \limitinf \frac{n^{\underline{k}}}{n^k}\rbracket{1-\frac{\lambda}{n}}^{n} \rbracket{1-\frac{\lambda}{n}}^{-k}\\
	&= \limitinf \frac{n^{\underline{k}}}{n^k}\limitinf\rbracket{1-\frac{\lambda}{n}}^{n} \limitinf\rbracket{1-\frac{\lambda}{n}}^{-k}\\
	&= 1\cdot e^{-\lambda}\cdot 1 = e^{-\lambda}
\end{align*}

\emph{A r.v. cannot be both continuous (not necessarily absolutely continuous) and discrete.}

\begin{itemize}
	\item A discrete r.v. takes probability 1 on a countable set $C$. Equivalently, there is a countable set $C$ such that $\sum_{c\in C}p(c) = 1$ for some function $p$, such that $P(X=c) := p(c)$.
	\item A continuous r.v. has a continuous cdf.
\end{itemize}

Suppose $X$ is discrete. Let $C$ be a countable set for which $P(X\in C) = \sum_{c\in C} p(c) =  1$. Clearly, it is not possible for all $p(c) = 0$, so $\exists c\in C, p(c) > 0$. Let $F$ be the cdf of $X$. Then we claim that $F$ is left-discontinuous at $c$. Because $\forall \epsilon, F(c-\epsilon) < P(X < c) = F(c) - p(c)$. $\qedsymbol$


\emph{Non-independent but covariance $0$.} Consider $Y = 1, -1$ with probability $1/2$ each. Then,
\begin{itemize}
	\item Given $Y = 1$, $X = 1$ with probability $1/2$ and $0$ otherwise
	\item Given $Y = -1$, $X = 2$ with probability $1/4$ and $0$ otherwise
\end{itemize}
The idea of $Y$ is clear, the positive half and negative half cancel out. The idea of $X$ is to vary its behavior given differing value of $Y$, but not too much, so that the positive part of $XY$ cancels out the negative parts, with their respective probability weights.

Note: The paragraphs in this document correspond to subsections in the textbook. For e.g. \textbf{4.2} corresponds to Chapman-Kolmogorov Equations.

\paragraph{4.1}\mbox{}\\
We have $P_{i,j} = P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1},\dots,X_0=i_0)$ for all $i_0,\dots,i_{n-1}$ in the state space. Hence, $P(X_{n+1}=j|X_n=i)=\sum P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1},\dots,X_0=i_0)P(X_{n-1}=i_{n-1},\dots,X_0=i_0) = P_{i,j}$.

Hence, conditioning on $X_n=i$, the events $\{X_{n+1}=j\}$ and $\{X_{n-1}=i_{n-1},\dots,X_0=i_0\}$ are independent.

\paragraph{4.2}\mbox{}\\
Claim: Consider the case of a state space with absorbing states $A$. Suppose states $i,j\notin A, k\in A$. Then $P(X_{n+m} = j\cap X_{n}=k | X_0 = i) = 0$. i.e. once state $k$ is reached, the probability of reaching any other state distinct from $k$ is $0$.

Since $P(X_{n+m} = j\cap X_n = k | X_0 = i) = P(X_{n+m}=j|X_n=k\land X_0=i)P(X_n=k|X_0=i)=P(X_m=j|X_0=k)P(X_n=k|X_0=i)$, it suffices to show that $P_{k,j}^m = 0$.\\
This can be verified inductively. We shall only show the inductive step. Suppose $P_{k,j}^\lambda = 0$ (in fact we need to assume the same for all $j$ distinct from $k$). Then $P_{k,j}^{\lambda+1} = \sum_{l}P_{k,l}^\lambda\cdot P_{l,j} = P_{k,k}^\lambda\cdot P_{k,j} = 1\cdot 0=0$.

\paragraph{4.3}\mbox{}\\
Another way to interpret $P_{i,j}^n$ is the following. $P_{i,j}^n > 0$ iff there is a sequence $l_1,\dots,l_{n-1}$ such that $P_{i,l_1},P_{l_1,l_2},\dots,P_{l_{n-1},j}\neq 0$. This is equivalent to saying $i\rightarrow l_1\rightarrow l_2\rightarrow\dots\rightarrow l_{n-1}\rightarrow j$. We can also let $l_0 := i, l_n := j$ and obtain a sequence $l_0,\dots,l_n$.

To take example 4.12 in this section as an example: Suppose we want to show that states $0,1$ cannot access state $2$. One possible argument to make is to consider the candidates for states $l:=l_{n-1}$ (for any $n$) such that $l_{n-1}\rightarrow 2$, or equivalently, $P_{l,2} > 0$. But looking at the 3rd column of the Markov matrix, we see that the only candidate for $l$ is $2$ itself. So it is impossible to have a series of state transition from $0$ or $1$ that ends up in $2$. (A formal proof can involve induction on the number of state transitions.)

\divider

\emph{Characterization of recurrent and transient states}
For a state $i$, let $f_i$ be the probability that state $i$ is revisited, given that the current state is $i$.\\
Let $Y_i\sim Geom(p=(1-f_i))$ be a r.v. counting the number of times state $i$ was entered, given that the starting state is $i$.
Transient states:
\begin{itemize}
    \item Have a $f_i$ value $<1$
    \item Have a finite expectation $E[Y_i]$ 
\end{itemize}
Recurrent states:
\begin{itemize}
    \item Have $f_i = 1$
    \item Have an infinite expectation $E[Y_i]$
\end{itemize}
Hence to argue that in a finite state space, there must be at least 1 recurrent state, we argue that the sum of expectation of all $Y_i$ must be infinite, so there must be some $i$ for which $E[Y_i] > 0$.

\divider

\emph{Corollary 4.2} We provide a more succint derivation of this result.
\begin{itemize}
    \item State $i$ recurrent implies $\sum_n p_{i,i}^n=\infty$
    \item $i\leftrightarrow j$ implies $\exists m_1,m_2$, $p_{i,j}^{m_1} > 0\land p_{j,i}^{m_2} > 0$
\end{itemize}
Hence,
\begin{align*}
    \sum_n p_{j,j}^n\geq \sum_{n\geq m_1 + m_2}p_{j,j}^n \geq \sum_{n\geq m_1 + m_2}p_{j,i}^{m_2}p_{i,i}^{n-m_1-m_2}p_{i,j}^{m_1} = p_{j_i}^{m_2}p_{i,j}^{m_1}\sum_n p_{i,i}^n
\end{align*}
The last sum is unbounded and we are done.

\end{document}
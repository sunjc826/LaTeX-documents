December 2021

## Notation
In backtick-quoted code, equality checking, comparisons etc follow the Java style, e.g. `==` for equality, `<=,>=` for comparisons.
In dollar-quoted maths (KaTeX), equality checking, comparisons will follow usual mathematical notation.

Furthermore, a mathematical statement like $x=1$, would in code form be `x==1`. This is to be taken as a declaration that $x$ equals $1$ is a true statement, unless it is explicitly mentioned for e.g., that `x==1` does not hold, or that `x==1` evaluates to false.

Assignment in maths is denoted by $\gets$ or $:=$.

## Mutual Exclusion Problem

### Properties
- Mutual Exclusion, Progress, No starvation (NOT bounded wait!)
- No starvation implies progress
- Progress and bounded wait implies no starvation

For example, in the following attempt to achieve mutual exclusion between 2 processes, there is bounded wait, but not progress. Additionally, a process can still get starved.

```typescript
// shared variables
let turn: number;

function RequestCS(i: number) {
    assert(i == 0 || i == 1);
    while (turn == (1 - i)) {}
}

function ReleaseCS(i: number) {
    assert(i == 0 || i == 1);
    turn = 1 - i;
}
```

### Peterson's Algorithm

#### Mutual Exclusion Property
In the proof by contradiction, we consider the most recent executions of both processes. For instance, process 0 is in the CS (critical section), so the most recent lines of code executed (barring the CS itself) is `RequestCS(0)`. Similarly, since process 1 is in the CS, the most recent lines of code executed falls under `RequestCS(1)`.

Note that we did not claim that the most recent executions **across all processes** are `RequestCS(0)` and `RequestCS(1)`. It could, for example be in the following order:
1. P(rocess)0: `RequestCS(0)`
2. P1: `RequestCS(1)`
3. P0: Enter CS
4. P0: `ReleaseCS(0)`
5. P0: `RequestCS(0)`
6. P0: Enter CS
7. P1: Enter CS
So, clearly, the most recent lines executed outside of the CS are actually `ReleaseCS(0)` and `RequestCS(0)`.

But the point is, when P0 and P1 are in the CS, the most recent lines executed by $P_i$ must be `RequestCS(i)`.

With this detail established, we continue on with the proof.

The variable `turn` can only take the values in $\{0,1\}$, and we can prove this since `turn` is initialized to $0\in \{0,1\}$ and in the program, `turn` can only ever be assigned to `0` or `1`.

We transcribe the proof given in lecture slides. By symmetry, we only discuss the case where `turn == 0`. (Note that when we say P0 executed `turn=1` we are referring to the most recent execution of that line by P0; similarly for other commands.)

Before reaching the CS, P0 must first execute `turn=1` and P1 must first execute `turn=0`. Since `turn==0`, of these 2 assignments, the (globally) most recent assignment must be `turn=0` by P1. Hence, between the time P1 executes `turn=0` and **now**(i.e. both P0, P1 in CS), P0 will not execute any line before `turn=1` (inclusive).

Hence, when P0 reaches the while loop, we are assured that `turn==0` as there are no further assignments to `turn`. Furthermore, for P1 to enter the CS, we must have `wantCS[0]==true` evaluating to false, i.e. `wantCS[0]==false`.

Here, we introduce arrow notation as follows. Event A $\rightarrow$ event B means that A took place prior to B.
P0 `wantCS[0]=true` $\rightarrow$ P0: `turn==1` $\rightarrow$ P1: `turn=0` $\rightarrow$ P1: while loop
However, based on this timeline analysis, we see at the time of P1's while loop the most recent assignment to `wantCS[0]` was by P0, setting it to `true`, a contradiction. $\square$

#### Progress Property
Suppose at least one of P0 and P1 are waiting. Then the only reason P0 and P1 may not enter the CS is due to the while loop, since all other lines in `RequestCS` can be run in constant (in particular, bounded) time, i.e. it suffices to consider the case where a process is either
1. at the while loop if waiting, 
2. or completely outside `RequestCS` if not waiting.

Consider cases, where we split the cases by how many processes are waiting. The lecture slides seem to only consider the first case.

Case 1: Both processes are waiting.
Progress holds by the fact that it is not possible for `turn==0` and `turn==1` to both be true.

Case 2: Only P0 is waiting for an indefinite period of time. Note our use of "indefinite", so that we can assume that P1 is not between the start of `RequestCS` and the end of `ReleaseCS` for all but a finite amount of time. We claim that if P1 is outside of this `RequestCS --- ReleaseCS` section, then `wantCS[1]==false`, so that P0 can pass the while loop and enter the CS. This is true since `wantCS[1]` is initialized to `false` and whenever P1 enters the `RequestCS --- ReleaseCS` section, P1 always sets `wantCS[1]` back to `false`, so that when P1 exits, `wantCS[1]` is `false`.

Case 3: Symmetric to Case 2.

Hence progress. $\square$

#### No Starvation Property
WLOG, we show that if P0 is waiting, P0 will not be starved. It suffices to show that $\exists k\in\mathbb{N}$ such that at most $k$ *other* processes distinct from P0 (in this case P1) that can enter the CS before P0.


Note that we do not need to prove the case where P0 is the only process waiting, since progress implies P0 must be able to enter CS. Hence, we now prove bounded wait.

There are some subtleties here. We actually need to establish a reference timepoint with respect to the bound on number of times P1 enters CS. Our reference timepoint here will be the time at which P0 is at the while loop.

Similarly to the start of the proof of progress, we assume that P0 is at the while loop, where the loop condition evaluates to `true`. We claim that $k=2$ will suffice for the while loop to evaluate to false, allowing P0 to return from `RequestCS`.

When P0 is waiting, P0 will not make any changes to the `turn` variable. While it may seem that $k=1$ is good enough, to be very rigorous, we should say $k=2$. This is because the first time P1 enters the CS, P1 may already be at the `while` loop, below the line `turn=0`. (So usually, $k > 1$. We can also define bounded wait to only include number of times other processes can enter CS **starting from RequestCS**. In that case, then $k=1$ would be fully correct.)

We now consider the 2nd time P1 tries to enter CS, starting from calling `RequestCS`, thereby setting `turn = 0`, in particular, P1 can only set change `turn` to `0`. The next time P1 calls `RequestCS` again, `turn == 0 && RequestCS[0] == true` so that P1 will block at the while loop. Hence, as long as P0 continues to run at this new timeframe, P0 can enter the CS.

Remark: It is actually not important whether P1 blocks or not (since this is relevant to mutual exclusion, but we are only talking about starvation here). As long as `turn == 0`, P0 will pass the while loop. $\square$

### Lamport's Bakery Algorithm
We note the following in the algorithm.
- Tuples `(queueNumber, id)` are ordered first by `queueNumber`, then `id`.
- In "get a number", $number[myid]\gets\max\{number[j] : 0\leq j<n\}+1$

#### Progress Property
Suppose a non-zero number of processes, say $1\leq k\leq n$ are waiting at `RequestCS`. Examining the code, we see that a process can only wait at one of the while loops in the second for loop.

Note that if a process is not waiting, i.e. $P\notin\{P_{i_1},\dots,P_{i_k}\}$, we can simply assume that it is completely outside `RequestCS` such that `choosing[process_id]==false` and `number[process_id]==0`.

Suppose a process, $P = P_i\in \{P_{i_1},\dots,P_{i_k}\}$ is one of the waiting processes. We consider cases.
1. P is waiting at `while (choosing[j]==true)` for some $j$. In this case, process $P_j$, which we will write as Pj can always complete the "get a number" portion, such that in a finite amount of time, `choosing[j]=false` executes. Then, `choosing[j]==true` evaluates to false under process P, or otherwise, we must have the assignment `choosing[j]=true`, which would mean Pj entered the CS, left the CS, and called `RequestCS`. While the 2nd case should not happen based on the code, even if it does, we already have progress.
2. After considering case 1, we can now assume that *all* waiting processes are now waiting at the second while loop, `while (number[j]!=0 && Smaller(number[j], j, number[i], i)`. In this case, we claim that we can find some $P_i\in \{P_{i_1},\dots,P_{i_k}\}$ such that $\forall j\in\{1,\dots,n\}$, `number[j]!=0 && Smaller(number[j], j, number[i], i)` evaluates to `true`, which we will prove below.

Remark: we can assume *"all waiting processes"* since there are finitely many processes, so that it takes a finite amount of time for all processes that will ever wait in this timeframe to be blocked at the 2nd while loop. 

**Lemma.** $\exists i\in\{1,\dots,k\}, \forall j\in\{1,\dots,n\}$, `number[j]!=0 && Smaller(number[j], j, number[i], i)` evaluates to true.

If $j\in \{1,\dots,n\}\setminus\{i_1,\dots,i_k\}$, then `number[j]==0` and so `number[j]!=0` evaluates to `false`.

Otherwise, amongst the elements `(number[index], index)` indexed by $\{i_1,\dots,i_k\}$, there must exist a minimal element, since there are finitely many elements. If $i_l$ indexes a minimal element, then process $P=P_{i_l}$ will have `Smaller(number[j], j, number[i_l], i_l)` evaluate to `false` for each $j\in\{1,\dots,n\}$.

Hence such a process P will pass both while loops at each iteration of the for loop and enter the CS. (in particular, we have shown that at least one waiting process can enter the CS.) $\square$

#### No Starvation Property
We fix a process of id $i\in\{1,\dots,n\}$, and that $P=P_i$ is waiting to enter the CS. We claim that P will not be starved.

As proven in progress, P will not be blocked by `while (choosing[j] == true)`.

At the current point of discussion, we sort the elements $\{(number[j], j) : 1\leq j\leq n \land number[j]\neq 0\}$. Then suppose $i$ is at position $p$. Then we claim that at most (in fact, exactly) $p-1$ processes can enter the CS before P does, i.e. $k = p-1$. (This is an inductive argument)

WLOG, we assume that all $p-1$ processes ranked ahead of $i$ by our ordering have entered and left the CS. At this point, regardless of what the other processes do, $(number[i], i) = \min\{(number[j], j) : 1\leq j\leq n \land number[j]\neq 0\}$, since "get a number" will grant any process entering `RequestCS` a queue number $\geq number[i] + 1$.

Hence, P will not be blocked by `while (number[j]!=0 && Smaller(number[j], j, number[i], i)` for any $j\in\{1,\dots,n\}$. $\square$

#### Mutual Exlusion Property
Suppose more than one process is in the CS. Choose any two of them, and label them P0, P1. Let the index of P0 be $k$ and the index of P1 be $l$. WLOG, $(number[k], k) < (number[l], l)$.

Consider the timeframe where P1 is at the second while loop, in particular, when P1 is comparing against P0. Since P1 passes this while loop, either `number[k] == 0` is true or (`number[k] != 0` true, `Smaller(number[k], k, number[l], l)` is false).
1. `number[k] != 0` true and `Smaller(number[k], k, number[l], l)` is false: This case is not possible since $(number[k], k) < (number[l], l)$, in the event that P0 is at its most recent call of `RequestCS`, plus P0 has executed `if (number[k] > number[l])`. Then P1 must get stuck here.
    If P0 has not reached this point, as long as P1's `number[l]` remains the same, the new number that P0 gets must be strictly greater than `number[l]`, because P0's number will be $\geq number[l] + 1$, a contradiction.
2. `number[k] == 0`: At this timeframe, P0 must be before the line `number[myId]++`, since this line will result in `number[k] > 0`. Furthermore, P0 must be in the loop, with loop variable `j > l` (or more precisely, P0 has executed the check `if (number[j] > number[myId])` where `j==l`). If not, P0 will get a number strictly greater than `number[l]`.

Next, consider an earlier timeframe, where P1 is at the first while loop. We further note that P1 manages to pass the first while loop, so that `choosing[k] == false`. There is only 1 possibility for this to occur, this is because `choosing[k]` can only evaluate to true within the "get a number" section. P0 must be at some point strictly (chronologically) before the most recent invoation of the `choosing[myId] = true` line, this also takes into account earlier invocations of `RequestCS(0)` (e.g. where P0 is after `choosing[myId] = false`). We conclude that P0 must then evaluate `number[j] > number[myId]` with $j = l, myId = k$ and `number[k]`. Contradiction. $\square$


## Synchronization Primitives
Useful link: https://stackoverflow.com/questions/37026/java-notify-vs-notifyall-all-over-again

From [javadocs on Object](https://docs.oracle.com/javase/10/docs/api/java/lang/Object.html#notifyAll())

```java
public final void notifyAll()
```
Wakes up all threads that are waiting on this object's monitor. A thread waits on an object's monitor by calling one of the wait methods.
The awakened threads will not be able to proceed until the current thread relinquishes the lock on this object. *The awakened threads will compete in the usual manner with **any other threads** that might be actively competing to synchronize on this object; for example, the awakened threads enjoy no reliable privilege or disadvantage in being the next thread to lock this object.*

- In other words, when notify or notifyAll are called, we can view this as the awakened thread(s) moving from the waiting queue to the monitor queue.

### Monitor (Java)
A process/thread can be in one of 3 states with regard to a synchronized method (monitor)
1. The thread is actively running. The monitor ensures that at most 1 thread can be in this state.
2. The thread is blocked. This can happen in 2 places.
   1. The thread calls the synchronized method and is attempting to acquire lock.
   2. The thread has been waken up from waiting state and is attempting to reacquire lock.
3. The thread is waiting, i.e. after a call to `wait`, the thread joins the waiting queue.

Useful link: https://stackoverflow.com/questions/5798637/is-it-safe-to-call-a-synchronized-method-from-another-synchronized-method

```java
void synchronized method1() {
    method2()
}

void synchronized method2() {
}
```

equivalent to 
```java
void method1() {
    synchronized (this) {
        method2()
    }
}

void method2() {
    synchronized (this) {

    }
}
```

Assume both methods are in the same class, so that `this` refers to the same object instance.
In a thread, when `method1` calls `method2`, the lock associated with `this` is already acquired, so the thread will not block.

### Producer/Consumer Problem
Pseudo-code from lecture slides
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isFull())
            sharedBuffer.wait();
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull();
        removeItemFrom(sharedBuffer);
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
We analyze this by cases.
#### One consumer, one producer
We claim that the solution is correct. We need to show that
1. When the buffer is full, producer does not write to it.
2. When the buffer is empty, consumer does not read from it.
3. Progress: Consumer can read from a non-empty buffer. Producer can write into a non-full buffer.

Suppose producer added an item to a full buffer, i.e. the producer executed
```java
        addItemTo(sharedBuffer);
```
when the buffer is full. (Call this time T1) We consider an earlier time point T2 where the producer most recently evaluated `sharedBuffer.isEmpty()`. Consider cases.
1. Suppose the evaluation is `false`, then somehow, an item got added into the buffer (between T2 and T1) without the producer having done anything. This is impossible, since only the producer is capable of adding items, and we only have one producer.
2. Suppose the evaluation is `true`, then the producer would enter the wait queue. The only way for the producer to wake up from the wait queue is if the consumer process calls `sharedBuffer.notify();`. We shall study whether it is possible for the buffer to still be full at this point in time.
   Denote T3 as the time the consumer calls `sharedBuffer.notify();`.
   Denote T4 as the time the consumer last calls `removeItemFrom(sharedBuffer);`. Note that T4 < T3 < T1. T2 is somewhere before T3.
   We claim that T2 < T4. Suppose not, then T4 < T2, but this says that $T4 < T2 < T3 < T1$, which implies that there are 2 processes inside the monitor-locked region, since consumer process is not blocked (since `removeItemFrom(sharedBuffer);` is strictly after the `wait`). Contradiction.
   Hence, $T2 < T4 < T3 < T1$. In other words, an item was removed from the buffer at T4, and the buffer remains untouched since then. In particular, the buffer is non-full.

We have a contradiction in both cases. Hence, it is impossible for a producer to produce into a full buffer.

Proving that the consumer does not consume from an empty buffer should be similar.

**Progress**
Next, we prove progress for producers. Claim: When the buffer is non-full, the producer can write to it in finite time, regardless of what the consumer does.
Clearly, if the producer is not blocked in the wait queue, then the producer can definitely produce for a non-full buffer. Hence, in order to obtain a contradiction, we assume that the producer is somehow in the wait queue despite a non-full queue.

Define T0 as the time point where the producer is in the wait queue, and *the buffer is not full*.
Define T1 as the time point where producer last executed `if (sharedBuffer.isFull())`. This must have evaluated to `true`, since the producer is now blocked.
So between T1 and T0, something must have happened to the buffer such that it is not full, i.e. a consumer removed an item from the buffer at some time T2, where $T1 < T2 < T0$.
Whether the consumer process then notifies the producer on the wait queue depends on whether `if (wasFull)` evaluates to `true`. Hence, define T3 as the time point where consumer executes `boolean wasFull = sharedBuffer.isFull();`. If $T1 < T3$ we are done.

Suppose that $T3 < T1$, this is not possible since we have 2 active processes inside a monitor.

```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isFull()) // T1
            sharedBuffer.wait(); // T0
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull(); // T3
        removeItemFrom(sharedBuffer); // T2
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
To be precise,
- producer is active in the time range $[T1, T0]$ and
- consumer is active at the time range $[T3, T2]$
- where $T1\in [T1, T0]\cap [T3, T2]$
there is an non-empty intersection in their active time periods, which contradicts the mutual exclusion guarantee of a monitor.

#### One consumer, multiple producers
A common point of failure with regard to the usage of monitors the that `notify` has no control over which process is woken up. Furthermore, the process that is woken up also competes with incoming processes.

We come up with a situation in which the synchronization mechanism fails.

Suppose the buffer size is $n\in \mathbb{Z}^+$. Assume that the buffer is full, after producers have written to it.
- 1 producer P1 gets blocked in the wait queue
- 1 consumer enters, removes item, and notifies.
- producer P1 wakes up, but does not manage to acquire monitor lock. Instead, an incoming producer P2 enters and acquires lock. P1 now waits in the monitor queue.
- P2 sees that `sharedBuffer.isFull() == true` and produces for the buffer. *Note that the buffer is now full again.*
- P2 exits monitor, P1 who is in the monitor queue then dequeues and adds another item to the queue. The algo therefore fails.

One particular point of failure is the use of `if` statement instead of `while`.

A similar case can be made for the multiple consumer, one producer case.

**Change**: Change the `if` to `while`
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isFull())
            sharedBuffer.wait();
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull();
        removeItemFrom(sharedBuffer);
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
We now attempt a different way to make the algo fail. Consider the following sequence of events. Note that the buffer is of capacity $n$.
- There are $n+2$ producers. $n$ producers write to the buffer, the remaining $2$ producers waits in the queue.
- There are $2$ consumers. The first consumer wakes up a producer P1. But when P1 wakes up, it does not manage to acquire lock. Instead, the second consumer acquires lock first.
- Now, 1 producer P2 is in the monitor queue, despite the fact that the buffer is not full (since P1 produces 1 item, and the 2 consumers consume 2 items)

The point of failure is that `notify` only wakes 1 process up. (i.e. one of the 2 sleeping producers). Only the first consumer manages to call notify. The second one does not since buffer is no longer full.

**Change**: Change `notify` to `notifyAll`
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isFull())
            sharedBuffer.wait();
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notifyAll();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull();
        removeItemFrom(sharedBuffer);
        if (wasFull)
            sharedBuffer.notifyAll();
    }
}
```
This is probably correct. Idk.

**Change**: Remove the conditional
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isFull())
            sharedBuffer.wait();
        addItemTo(sharedBuffer);
        sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        removeItemFrom(sharedBuffer);
        sharedBuffer.notify();
    }
}
```

While this may seem correct at first, these two are not equivalent.
i.e.`if (wasEmpty) { sharedBuffer.notifyAll(); }` and `sharedBuffer.notify();`. The main problem, again, is that notify has no control over which process is woken up, so the producer calling `notify` is not guaranteed to wake up a consumer process. The producer may wake up another waiting producer, and we can easily construct a concrete sequence of events to demonstrate failure.

Assume that the shared buffer of size $n$ is full.
- $2n$ producers enter and block.
- $2n$ consumers enter. $n$ of them wake up $n$ producers. The other $n$ block.
- Now there are $n$ producers and $n$ consumers in the queue, and another $n$ producers that are awake.
- The $n$ awake producers produce items for the buffer and make it full again. However, unfortunately, when `notify` is called, only producers are woken up.
- We now have a full buffer, $n$ awakened producers and $n$ waiting consumers.
- The awakened producers would go back to the waiting queue as `sharedBuffer.isFull()` evaluates to `true`.
- We now have a full buffer, $n$ waiting producers and $n$ waiting consumers. Despite the full buffer, all the consumers are waiting.

Hence, this fails.

### Reader-Writer problem
To prove correctness, we need to show the following:
1. When a writer is writing to a file F, no reader can read, and no other writer can write.
2. When (possibly multiple) readers are reading, writers cannot write.

More lemmas to prove:
- When reader calls `notify`, there can be no reader in the wait queue.

Extensions:
- Devise a no starvation algorithm.

```java
void writeFile() {
    synchronized (object) {
        while (numReader > 0 || numWriter > 0)
            object.wait();
        numWriter = 1;
    }
    // write to file;
    synchronized (object) {
        numWriter = 0;
        object.notifyAll();
    }
}

void readFile() {
 synchronized (object) {
        while (numWriter > 0)
            object.wait();
        numReader++;
    }
    // read from file;
    synchronized (object) {
        numReader--;
        object.notify();
    }
}
```

#### Correctness
We prove **1**. Suppose a writer W1 is writing to file F. Consider cases.
1. Suppose another writer W2 is writing to F as well. We define time point T1 where W1 most recently executes `while (numReader > 0 || numWriter > 0)`, and time point T2 such that W2 most recently executes `while (numReader > 0 || numWriter > 0)`. WLOG, $T1 < T2$. Then right before T2, `numWriter == 1`, since T2 can only occur when W1 has left the monitor block, so that `numWriter = 1` has executed. Hence, W2 will be blocked by the while loop and join the wait queue instead. Contradiction.
2. Suppose another reader R is reading from F. We define time point T1 where W1 most recently executes `while (numReader > 0 || numWriter > 0)` and time point T2 where R most recently executes `while (numWriter > 0)`. We consider cases.
   1. If $T1 < T2$, this is not possible since `numWriter == 1` at the time of T2 and reader R will get blocked.
   2. If $T2 < T1$, this is also not possible since `numReader++` has been executed by R by the time of T1. Hence, W1 gets blocked at T1. There is a small detail here. We need to show that `numReader` is always non-negative, so that when `numReader++` executes, it becomes positive. To show this, we observe that we can pair up the `++` and `--`.
   We have thus attained a contradiction in both subcases.
Hence, we have proven **1**. $\square$ 

We prove **2**. Suppose at least 1 reader R is reading, and a writer W is writing to F. This is actually the same as case 2 of **1**. So we have this proven for free.

**Lemma** When reader calls `notify`, there can be no reader in the wait queue.
Let R1 be the reader calling `notify` at time point T1 and we suppose that at T1 there exists a reader R2 in the wait queue.

```java
void writeFile() {
    synchronized (object) {
        while (numReader > 0 || numWriter > 0)
            object.wait();
        numWriter = 1;
    }
    // write to file;
    synchronized (object) {
        numWriter = 0;
        object.notifyAll();
    }
}

void readFile() {
 synchronized (object) {
        while (numWriter > 0) // T2: R2, T3: R1
            object.wait(); // R2 waiting
        numReader++;
    }
    // read from file;
    synchronized (object) {
        numReader--;
        object.notify(); // T1
    }
}
```
Let T2 be the time R2 most recently executed `while (numWriter > 0)` and let T3 be the time R1 most recently executed `while (numWriter > 0)`. We consider cases. Note that it is clear that $T2, T3 < T1$.
1. Suppose $T3 < T2 < T1$. At T3, `numWriter == 0` since R1 passes the while loop and at T2, `numWriter > 0` since R2 get blocked. Hence, this says that there is an incoming writer W in the time range $(T3, T2)$. Let T4 be the time W entered the monitor. We see that $T3 < T4 < T2$. W, seeing that `numReader > 0`, enters the wait queue and is unable to increment `numWriter`. The same holds even if more than 1 writer enters. Hence, it is impossible that `numWriter > 0`. Contradiction.
2. Suppose $T2 < T3 < T1$. At T2, `numWriter > 0` and at T3, `numWriter == 0`. Let W be a writer present in the time interval $(T2, T3)$. W writes and executes `notifyAll`. It is possible that another writer wakes up and acquires lock, but WLOG, we assume that W is the last writer to acquire lock **before T3**, i.e. after this it must be a reader that acquires lock. Let T4 be the time point W exits the monitor. At this point, no process can be in the wait queue, since all are woken up by `notifyAll`. In particular, R2 is awake. Since the next processes that acquire lock are all readers, this also includes R1. When R1 eventually reaches T1, there are no readers in the wait queue. This completes the proof. 
$\square$ 

**Corollary** If a reader notifies a process, that process must be a writer.

**Anti-Starvation** Done in homework document.

## Consistency Conditions
We use the following notation, if not otherwise specified
An event $e$ takes the form
$$\texttt{inv/resp(process, operation\_type, resource, value)}$$
- Value may be omitted if no value is involved in the operation

An operation $o$ is a pair of events $e, e'$, where $e$ denotes the invocation event and $e'$ denotes the response event. The properties associated with an operation are retrievable as follows:
- Invocation event $e = inv(o)$
- Response event $e' = resp(o)$
- Process $proc(o)$
- Resource $res(o)$

A history $H$ implies a partial order $<_H$, where $o_1 < o_2 \iff resp(o_1) < inv(o_2)$. (Inequalities between events refer to the physical time at which they occur.)
This is known as the external order, or occurred before order.

**Remark** Consistency conditions relate to parallel systems, not distributed systems. The difference is that a parallel system is located in a single computer, where it is possible for a single physical clock to dictate what is the physical time. Whereas in a distributed system, there are many computers, some remotely located, so that there are multiple physical clocks with varying degrees of accuracy, so it is near impossible to have a single physical time. In the next chapter, we see that logical clocks are used instead. 

### Sequential history
Intuitively, a (legal) sequential history is one in which it seems like a single process is creating invoke events and receiving response events.

**Proposition** These 2 definitions of sequential history $(H, <_H)$ are equivalent.
1. $<_H$ is a total order on operations, such that for all operations $o_1, o_2$, $o_1 <_H o_2\lor o_2 <_H o_1$.
2. Every invocation of an operation $o$, $inv(o)$, must be immediately followed by its response, $resp(o)$, i.e. $H$ is of the form $inv(o_1), resp(o_1), inv(o_2), resp(o_2), \dots$

Clearly, **2** implies **1**. We shall only prove **1** implies **2**.
Suppose **1**. We then conduct induction on $H$, let $H$ have $n$ operations, so that the length of $H$ is $2n$. WLOG, let the first entry of $H$ be $inv(o_1)$. (Clearly, the first entry of $H$ cannot be a response event).
Suppose the second entry is not $resp(o_1)$, then it is another invocation event, say $inv(o_2)$. But this means that $o_1$ and $o_2$ are not comparable, since it is neither the case that $inv(o_2)$ follows $resp(o_1)$ nor is it the case that $inv(o_1)$ follows $resp(o_2)$. $\square$
#### Process order
A clearer definition of process order is the sub-partial order imposed by $\bigcup_{\text{process } p} <_{H|p}$.

A legal sequential history $S$ for a sequentially consistent history $H$ preserves process order, but not necessarily external order.
### Definitions of Linearizability
Claim: The 2 definitions of linearizability are equivalent.
Reformulation of definition 1: For a history $H = \{o_1,o_2,\dots,o_n\}$, where $o_i$ denotes an operation, $\exists$ choice function $c: \{1,2,\dots,n\}$ such that we have the corresponding "degenerate" history $\{c(o_i)\in [inv(o_i), resp(o_i)] : i\in \{1,2,\dots,n\}\}$, where for each $i$, $inv^*(o_i) = resp^*(o_i) = c(o_i)$, giving a degenerate legal sequential history $H^* = (inv^*(o_1), resp^*(o_1),inv^*(o_2), resp^*(o_2),\dots,inv^*(o_n), resp^*(o_n))$.

We call $H^*$ degenerate since it is technically not possible for events to take place at the same time.

Definition 2: A history $H$ for which there exists a legal sequential history $S$ that preserves the partial(external) order $<_H$ given by $H$. i.e. $<_H\subseteq <_S$.

Note the 3 parts of this definition: $S$ needs to be
1. legal
2. sequential
3. preserves $<_H$

$(\implies)$ Suppose definition 1 is true for a history $H$. We then construct a sequential history $S_c=([c(o_i) - \epsilon, c(o_i) + \epsilon])_{1\leq i\leq n}$ where each operation takes arbitrarily short time $\epsilon > 0$ **AND** $inv(o_i) \leq c(o_i) - \epsilon, c(o_i) + \epsilon \leq resp(o_i)$. Since there are finitely many intervals ($n$ of them), it is possible to choose small enough $\epsilon$ so that all intervals do not overlap. Since intervals are disjoint, $S_c$ is sequential.
Furthermore, each interval $[c(o_i) - \epsilon, c(o_i) + \epsilon]\subseteq [inv(o_i), resp(o_i)]$ in terms of time, so that the partial order $<_H$ is preserved. For example, if $a <_H b$ (equivalent to $resp(a) < inv(b)$), then $c(a) + \epsilon < c(b) - \epsilon$ so that $a <_S b$. Hence, the partial order is preserved by $S_c$.
Like the "degenerate" legal history(call it $D$), $S_c$ preserves the chronological order of events. In other words, for any operation $o$, the events that have occurred prior to $o$ in $S_c$ are exactly the same as in $D$, and in the same order too. The applies for what occurs after. Hence, the legality of $D$ implies the legality of $S_c$, as relative chronology is the same in $D$ and $S_c$.
Hence, we have show that definition 2 holds as well.

*Remark*: We cannot just say $D$ is a sequential history as technically speaking, no 2 events can occur at *exactly* the same time (we can treat this as an axiom), so there is no such thing as an operation with invocation time equalling response time.

$(\impliedby)$ Suppose definition 2 is true for a history $H$ with $n$ operations. Let $S$ be the legal sequential history with the properties mentioned (legal, sequential, preserves $<_H$). Without loss of generality, (by possibly renumbering the operations), $S$ gives the sequence $(o_1, o_2,\dots,o_n)$, where each $o_i$ subsumes a pair of $inv$ and $resp$ events. We then construct our choice function as follows. In fact, we choose points for operations in ascending order from $1$ to $n$.

At each $i$, let $d_i := \min\{resp(o_j) - \max\{c(o_{i-1}), inv(o_i)\} : j \geq i \}$, where we define $c(o_0):=-\infty$. Certainly $d_i > 0$. The reason:
- $c(o_{i-1}) < resp(o_j)$ by inductive hypothesis
- $inv(o_i) < resp(o_j)$ since $o_i <_S o_j$ for $j > i$. Note that if there exists $j > i$ such that $resp(o_j) < inv(o_i)$, this would imply $o_j <_H o_i$, yet $o_i <_S o_j$ as we have assumed. This contradicts the fact that $<_H\subseteq <_S$.

Then let $c(o_i) := \max\{c(o_{i-1}), inv(o_i)\} + \frac{d_i}{n}$. Intuitively, this gives us freedom to define the points $c(o_{i+1}) = c(o_i) + \frac{d_i}{n}$, $c(o_{i+2}) = c(o_{i+1}) + \frac{d_i}{n}$ if need be.
Formally, this allows us to re-establish the inductive hypothesis, as 
$$c(o_i) = \max\{c(o_{i-1}), inv(o_i)\} + \frac{d_i}{n} < \max\{c(o_{i-1}), inv(o_i)\} + d_i \leq \min\{resp(o_j) : j > i\}$$

We then need to show this is legal. But the legality of this comes directly from the legality of $S$, since the order of operations of the degenerate history $S_c$ exactly follows that of $S$, precisely stated as $<_{S_c} = <_S$. So we are done. $\square$

**Example** (Slide 22):
Let x be an integer data type supporting read and write.
- inv(Q, read, x) $q$
- inv(P, write, x, 1) $p$
- resp(Q, read, 1)
- inv(R, read, x) $r$
- resp(P, write)
- resp(R, read, 0)

By rearranging the operations $r <_S p <_S q$ sequentially, we see that this history is sequentially consistent.
However, the history is not linearizable. Suppose a linearization $S'$ exists. Then we must have
- $q <_{S'} r$ since $q <_H r$
- $p <_{S'} q$ since $p$ is the only write operation that writes $1$

Hence, we get $p <_{S'} q <_{S'} r$. But this is not legal based on the sequential semantics of $x$, as we are reading $0$ after $1$ is written.

### Sequential consistency is not a local property
Given the events of history $H$ as follows:
1. inv(P, enqueue, x, 1) $p_1$
2. resp(P, enqueue, x)
3. inv(Q, enqueue, y, 2) $q_1$
4. resp(Q, enqueue, y)
5. inv(P, enqueue, y, 1) $p_2$
6. resp(P, enqueue, y)
7. inv(Q, enqueue, x, 2) $q_2$
8. resp(Q, enqueue, x)
9. inv(P, dequeue, x) $p_3$
10. inv(Q, dequeue, y) $q_3$
11. resp(P, dequeue, x, 2)
12. resp(Q, dequeue, y, 1)
where $p_i, q_i$ denote operations.

As seen in lecture, $H|x$ and $H|y$ are sequentially consistent. Now, we prove formally that $H$ is not sequentially consistent. Suppose not, such that there exists a legal sequential history $S$ for $H$.

Since $S$ must preserve process order, we must have $p_1 < p_2 < p_3\land q_1 < q_2 < q_3$ -- $(0)$. (Here $<$ denotes $<_S$)
- $p_1$ enqueues 1 into x, and $q_2$ enqueues 2 into x. Since $p_3$ dequeues 2 from x, we must have $q_2 < p_1$ -- $(1)$
- By a similar argument for y, we must have $p_2 < q_1$ -- $(2)$

We now have the following ordering which respects $(0)$ and $(1)$
$$q_1 < q_2 < p_1 < p_2 < p_3$$
But immediately we see a contradiction, since here, $q_1 < p_2$, yet $(2)$ states otherwise. This contradicts the fact that $S$ is a total order.

Hence $S$ cannot exist, that is, $H$ is not sequentially consistent.

### Linearizability is a local property
i.e. A history $H$ is linearizable **iff** for all objects $x$, $H|x$ is linearizable.

#### Using Definition 1

$(\implies)$ Given history $H$, suppose that it is locally linearizable. Let $x_1,x_2,\dots,x_m$ be the objects that appear in $H$. Restating the assumption, $\forall i\in \{1,\dots,m\}, H|x_i$ is linearizable, such that there is a choice function $c_i = o \mapsto c_i(o)\in [inv(o, x_i), resp(o, x_i)]$.

Let $c$ be the "union" of all $c_i$. Now it is not necessarily the case that $c$ is injective for all operations $o$ in $H$. But that can be easily resolved. For any 2 $o_1, o_2$ such that $c(o_1) = c(o_2)$, we "perturb" them arbitrarily, e.g. $c(o_1) := c(o_1) - \epsilon, c(o_2) := c(o_2) + \epsilon$ or vice versa, for some arbitrarily small $\epsilon$. This modified $c$ ensures that there are no clashes, i.e. $c$ is injective.

We claim that $c$ is now a legal (degenerate) linearization of $H$.

We note that the state of an object $x$ during an operation $o$ is solely dependent on:
1. the operations conducted on $x$ prior to $o$, and
2. the parameters passed to $inv(o)$.

Both of these don't change when we merge all the $c_i$ together to form $c$. For example, suppose $c_1(o_1) < c_1(o_2)$ and $c_2(o_3) < c_2(o_4)$ are 2 local linearizations of $H|x_1, H|x_2$ respectively. Suppose that merging them gives $c(o_1) < c(o_3) < c(o_2) < c(o_4)$, i.e $c(o_3)$ cuts between $c(o_1)$ and $c(o_2)$. But since all operations are unchanged, in particular, $o_1, o_3$ still pass the same parameters to $x_1$, the behavior of $x$ does not change just because there is $o_2$. - $(*)$
See lecture slide 29 for a similar explanation.

Hence, $c$ is a legal choice function, and by definition 1, $H$ is linearizable.

$(\impliedby)$ Conversely, suppose that $H$ is linearizable. Let $c$ be the choice function. For each object $x$, let $c_x$ be the restricton of $c$ to those operations involving object $x$. Similar to $(*)$, we see that restricting $c$ removes those operations that don't involve $x$, and because they don't involve $x$, their removal does not affect the legality of the sequence of events involving $x$. This proves legality, and hence $H$ is locally linearizable. $\square$

#### Using Definition 2
The lecture has proven the forward direction, so we will prove the converse.

$(\impliedby)$ Suppose $H$ is linearizable, with the linearization being $S$. We claim that for each object $x$, $S|x$ is a linearization of $H|x$.

First, since $S$ is a sequential history, so is $S|x$.
Next, $<_H \subseteq <_S$, so $<_{H|x} \subseteq <_{S|x}$. To see this, suppose $o_1 <_{H|x} o_2$, then $o_1 <_H o_2$, then $o_1 <_S o_2$, and finally $o_1 <_{S|x} o_2$.
Finally, $S$ is legal, so $S|x$ is also legal. The idea is again very similar to $(*)$ in the [previous section on Using Definition 1](#using-definition-1). $\square$


Here, we discuss some aspects of the lecture's proof in the forward direction.
**Lemma** Given a directed acyclic graph $G$ and a topological sort of graph $T$. Let $<_G$ be the partial ordering of vertices given by $G$. $<_T$ is then a total order such that $<_G \subseteq <_T$.

The proof is immediate from the definition of a topological sort. $\square$

We discuss how the topological sorting of the operation graph gives a linearization of $H$. Let $S$ be the sequential history given by the topological sorting.

First, $S$ is equivalent to $H$ since $S$ contains the exact same set of vertices.
Second, $S$ is legal as $S$ merely "splices" together several per-object sequential histories $S|x$, and as the [previous section on Using Definition 1](#using-definition-1) has shown, if $\forall x, S|x$ legal, $S$ is legal.
Third, $<_H \subseteq <_G$ where $<_G$ is the partial order formed by the union of $<_H \cup \bigcup_{\text{object } x} <_{S|x}$ (G here refers to the directed acyclic graph) and $<_G \subset <_S$ since $S$ is formed by topologically sorting $G$. Hence $<_H \subseteq <_S$ and $S$ respects the operational partial order.

Hence $S$ is a linearization of $H$. $\square$

Next, we give a detailed proof on why the graph is acyclic. Suppose not, such that there exists a cycle in the graph. We start by considering what sort of edges this cycle consists of.

We denote $S|x$ as the linearization of $H|x$. Note that as of this point, we don't really care what is $S$, in fact, we haven't even defined $S$. $S|x$ is purely a notation.

1. The cycle does not contain any edges from any $S|x$. i.e. all edges in the cycle are across different objects. But this means that the partial order $<_H$ has a cycle, which is a contradiction.
2. The cycle does not contain any edges across operations on different objects. This implies that the cycle only has edges in a single $S|x$. But $S|x$ is a sequential history, in particular $S|x$ is a history (i.e. $<_{S|x}$ is a partial order), and cannot have cycles.  
3. The cycle contains edges from both $S|x$ for various $x$ and across operations on different objects.

Since we have eliminated cases 1 and 2, only case 3 remains. As mentioned in the lecture, a cycle must be of the form
- edges in $S|x_1$
- edges across operations on different objects
- edges in $S|x_2$
- edges across operations on different objects
- ...
- edges in $S|x_n$
- edges across operations on different objects, closing the cycle

where $n\in \mathbb{Z}^+$.

**Lemma** A very trivial one. Suppose $<_H \subseteq <_S$ where $<_H$ is a partial order and $<_S$ is a total order. Then $<_S$ cannot "disagree" with $<_H$.

Suppose not, such that there exist $a,b$, $a <_H b$ but $b <_S a$. Then $a <_S b\land b <_S a$, so that $a = b$. This is a contradiction. $\square$

The above lemma justifies why multiple edges in $S|x_i$ for any $i$ in the cycle can be contracted to a single one.

For example, suppose for some $x$, in $S|x$, edges $o_1\rightarrow o_2\rightarrow o_3$ belong to the cycle. We contract this to just $o_1\rightarrow o_3$, equivalently, $o_1 <_{S|x} o_3$. Since $<_{S|x}$ cannot disagree with $<_{H|x}$, we cannot have $o_3 <_H o_1$. Equivalently, in terms of timestamps, we cannot have $resp(o_3) < inv(o_1)$, and this is precisely used in the timeframe analysis later on.

We transcribe the timeframe analysis in lecture precisely. This only considers a specific case where $n=2$. Suppose we have conducted the aforementioned contraction, and obtained the following cycle:
$D <_H A <_{S|x_1} B <_H C <_{S|x_2} D$
- Since $D <_H A$, $resp(D) < inv(A)$
- Since $A <_{S|x_1} B$, $\sim (resp(B) < inv(A))$ where $\sim$ denotes logical negation. This implies $inv(A) \leq resp(B)$, which implies $inv(A) < resp(B)$.
- Since $B <_H C$, $resp(B) < inv(C)$.
- Since $C <_{S|x_2} D$, $inv(C) < resp(D)$.

Combining these time orderings, we get
$$
resp(D) < inv(A) < resp(B) < inv(C) < resp(D)
$$
which is a contradiction as by transitivity $resp(D) < resp(D)$.

**Proposition** Linearization may not be unique.
We can consider an example where there are 2 processes $A, B$, with operations $a, b$ respectively that are not comparable, e.g.
$inv(a), inv(b), resp(a), resp(b)$

Furthermore, the operations $a, b$ act on distinct objects, so both of these linearizations are legal.
- $inv(a), resp(a), inv(b), resp(b)$
- $inv(b), resp(b), inv(a), resp(a)$

## Models and Clocks
The difference between this chapter and the previous (consistency conditions) is that the previous chapter considers a parallel system, where there is indeed a single physical clock (the system clock), but this chapter considers a distributed system, where systems can be geographically separated, so there can be no single clock.

The other difference (a consequence of the first) is that a total ordering of *events* (but not necessarily operations) is possible in the parallel case, but not so for distributed systems. Due to it being impossible or extremely difficult to synchronize clocks, slight differences in clocks lead to events very close together being not possible to order. For e.g., suppose we have 2 geographically separated systems S1 and S2. An event e1 occurs at time $t$ based on S1's physical clock, and an event s2 occurs at time $t+\delta t$ based on S2's physical clock. If $\delta t$ is sufficiently small (perhaps smaller than the relative inaccuracies between S1 and S2's clocks), can we truly say e1 occured before e2 based on the physical clock? The answer is no.

**Lemma** An interesting lemma from the textbook. Suppose $\rightarrow$ is a partial order (non-total), and $<$ is a total order. Suppose also that $\forall a, b, a\rightarrow b\implies a < b$. Then it cannot be true that $a < b\implies a\rightarrow b$.

The reason is that if $a < b\implies a\rightarrow b$ were to hold,  $\rightarrow$ would then be a total order.


### Vector clock
There is a difference between the lecture's version of the vector clock and the textbook's version.
They agree on these properties:
- For an intra-process event, `clock[pid]++`, where `pid` is the index associated with the process
- When receiving a message, the following is executed
```typescript
class VectorClock {
    const clock: Array<number>; // Vector of length n
    const pid: number;
    function receive(senderClock: Array<number>) {
        for (let i = 0; i < n; i++) { // component-wise maximum
            clock[i] = Math.max(clock[i], senderClock[i]);
        }
        clock[i]++;
    }
}
```

However, there is a difference when sending messages.
- The lecture's vector clock does `clock[pid]++` prior to sending the message. So the act of sending the message is similar to a regular intra-process event.
- Whereas the textbook's vector clock does `clock[pid]++` *after* sending the message.
The same can be said about the matrix clock algorithm.


Claim. For the vector clock protocol, given events s, t, $s\rightarrow t\iff T_s < T_t$, where $T$ is the vector clock logical time at the point of the event.

$(\implies)$ Consider cases.

1. s and t are both events on a single process $i$, and s is a computation event right before t. Then by definition of the vector clock protocol, $\forall j\neq i, T_s[j] = T_t[j]$ and $T_s[i] < T_s[i] + 1 = T_t[i]$, so that $T_s < T_t$.
2. s and t are events on different processes $i$ and $j$ respectively, and $s$ is send event that corresponds to $t$, a receive event. By definition of the vector clock protocol, taking the component-wise maximum of $T_s$ and the predecessor of $T_t$ and incrementing the $j$th component of the predecessor of $T_t$ means that $T_s < T_t$.
3. s happened before t via a chain of cases 1 and 2. Since $<$ between $T$ is transitive, and we already showed that $s\rightarrow t\implies T_s < T_t$ for cases 1 and 2, $s\rightarrow t \implies T_s < T_t$ for case 3 as well.

$(\impliedby)$ Suppose s is an event on process i and t is an event on process j. Suppose that $T_s < T_t$. In particular, $T_s\leq T_t$ and $T_s[i]\leq T_t[i]$. We observe that the ith component of a vector clock time $T$ can only be incremented by process i. Hence, if we were to search through all ancestors of event t via the happened before relation, we must be able to find s. Because otherwise, then any ancestor event belonging to process i will have its ith component strictly smaller than $T_s[i]$, and so $T_t[i] < T_s[i]$, which is a contradiction.

## Global snapshots
Global snapshots give a *possible* state in the past rather than what may actually have happened. Despite this, they are still useful in the following areas.
- Debugging distributed systems. Firstly it is not feasible to stop all remote systems at the same time. Next, given a global snapshot, even if the snapshot may not actually have happened, it can still be used to check the correctness of a distributed algorithm. For e.g. a distributed algorithm may give certain state guarantees that no global snapshot should violate.
- Checkpointing in long running computations. E.g. when running a distributed simulation, systems may crash. By rolling back to a previous snapshot, the computation can proceed from there. The interesting thing is that we might be rolling back to a snapshot that may not actually have happened (think multiverse!), but assuming that the distributed algorithm is coded correctly, the output should still be correct regardless of the global snapshot we roll back to.

**Definition** Given a set $S$ of events, a global snapshot $G\subset S$ is such that $\forall e\in G$, any ancestor of $e$ in the same process is also in $G$.

As a consequence, a global snapshot respects point 1 of the happened before relation.


**Definition** A consistent global snapshot is one that respects send-receive order.

i.e. such a snapshot respects point 2 of the happened before relation.

By inductive argument, a consistent global snapshot respects point 3 (transitivity) of the happened before relation.


**Proposition** Given a set of distributed processes, consider a single process P with events $e_1, e_2,\dots$. Then, for any $m\in \mathbb{N}$, we can find an upper bound as well as a lower bound to a consistent global snapshot containing $e_1,\dots,e_m$ and does not contain $e_{m+1}, e_{m+2}\dots$ - $(\star)$.

The lower bound $L$ is simply all ancestors of $e_m$ with respect to the happened before relation. To see this we can show that
- Any consistent global snapshot $G$ satisfying $(\star)$ must contain $L$, so that $L$ is a subset of the lower bound. This is trivial to show since $G$ respect the happened before order.
- $L$ is itself a consistent global snapshot. To show this, consider any event $f\in L$ and suppose $f'$ happened before $f$, i.e. $f'\rightarrow f$. Then, we have $f'\rightarrow f\rightarrow e_m$, so that $f'\rightarrow e_m$, so that $f'$ is an ancestor of $e_m$. By our construction, $f'\in L$.

The upper bound $U$ is the setwise complement of $e_{m+1}$ and all its descendants (we denote this as set $C$, so $U = C^c$). To see this we show that
- Any consistent global snapshot $G$ satisfying $(\star)$ must not contain $C$, so that $G\subseteq U$. This is trivial since containing any element of $C$ would not respect happened before order.
- $U$ is itself a consistent global snapshot. We consider any element $f\in U$ and suppose that $f'\rightarrow f$. We note that $e_{m+1}\not\rightarrow f$. Since $f'\rightarrow f$, $e_{m+1}\not\rightarrow f'$, and by construction, $f'\in U$.

We have thus established an upper and lower bound. $\square$

### Chandy and Lamport's protocol
Here are some characteristics of the protocol. Suppose there are $n$ distributed processes. Then,
- Each process will send out 1 Marker message to the other $n-1$ processes when it takes its local snapshot.
- Since each process sends $n-1$ Marker messages, there are $n(n-1)$ Marker messages associated with one global snapshot.
- It is not necessary for there to be exactly 1 process to initiate the protocol. For e.g., there may be more than 1 initiator. This can happen when more than 1 process tries to take a local snapshot before receiving any Marker messages.
- For a process, it can take its local snapshot either (1) before any Marker messages is received, (2) when the first Marker message is received.
- The local snapshot is instantaneous but processes would want to capture messages in transit. Because of this, each process will record down the messages received between its local snapshot and any Marker message that is received.
  For example, consider process 1, and it takes a local snapshot at time $t_1$, and receives Marker messages from process 2, 3, ..., n in this order. We let the time of reception of the ith Marker message be $t_i, i = 2,3,\dots,n$. Then process 1 will record down the following info: messages in $(t_1, t_2)$, messages in $(t_1, t_3)$, ... messages in $(t_1, t_n)$.
  By exampling the messages in $(t_1, t_5)$ for example, allows us to determine what messages were in transit between process 1 taking its local snapshot and receiving the Marker message from process 5.

## Message Ordering
In the previous 2 chapters, we only concerned ourselves with the happened before order (for e.g. a consistent global snapshot respects the happened before order). In this chapter, more orders are introduced.




















December 2021

## Notation
In backtick-quoted code, equality checking, comparisons etc follow the Java style, e.g. `==` for equality, `<=,>=` for comparisons.
In dollar-quoted maths (KaTeX), equality checking, comparisons will follow usual mathematical notation.

Furthermore, a mathematical statement like $x=1$, would in code form be `x==1`. This is to be taken as a declaration that $x$ equals $1$ is a true statement, unless it is explicitly mentioned for e.g., that `x==1` does not hold, or that `x==1` evaluates to false.

Assignment in maths is denoted by $\gets$.

## Mutual Exclusion Problem

### Properties
- Mutual Exclusion, Progress, No starvation (NOT bounded wait!)
- No starvation implies progress
- Progress and bounded wait implies no starvation

### Peterson's Algorithm

#### Mutual Exclusion Property
In the proof by contradiction, we consider the most recent executions of both processes. For instance, process 0 is in the CS (critical section), so the most recent lines of code executed (barring the CS itself) is `RequestCS(0)`. Similarly, since process 1 is in the CS, the most recent lines of code executed falls under `RequestCS(1)`.

Note that we did not claim that the most recent executions **across all processes** are `RequestCS(0)` and `RequestCS(1)`. It could, for example be in the following order:
1. P(rocess)0: `RequestCS(0)`
2. P1: `RequestCS(1)`
3. P0: Enter CS
4. P0: `ReleaseCS(0)`
5. P0: `RequestCS(0)`
6. P0: Enter CS
7. P1: Enter CS
So, clearly, the most recent lines executed outside of the CS are actually `ReleaseCS(0)` and `RequestCS(0)`.

But the point is, when P0 and P1 are in the CS, the most recent lines executed by $P_i$ must be `RequestCS(i)`.

With this detail established, we continue on with the proof.

The variable `turn` can only take the values in $\{0,1\}$, and we can prove this since `turn` is initialized to $0\in \{0,1\}$ and in the program, `turn` can only ever be assigned to `0` or `1`.

We transcribe the proof given in lecture slides. By symmetry, we only discuss the case where `turn == 0`. (Note that when we say P0 executed `turn=1` we are referring to the most recent execution of that line by P0; similarly for other commands.)

Before reaching the CS, P0 must first execute `turn=1` and P1 must first execute `turn=0`. Since `turn==0`, of these 2 assignments, the (globally) most recent assignment must be `turn=0` by P1. Hence, between the time P1 executes `turn=0` and **now**(i.e. both P0, P1 in CS), P0 will not execute any line before `turn=1` (inclusive).

Hence, when P0 reaches the while loop, we are assured that `turn==0` as there are no further assignments to `turn`. Furthermore, for P1 to enter the CS, we must have `wantCS[0]==true` evaluating to false, i.e. `wantCS[0]==false`.

Here, we introduce arrow notation as follows. Event A $\rightarrow$ event B means that A took place prior to B.
P0 `wantCS[0]=true` $\rightarrow$ P0: `turn==1` $\rightarrow$ P1: `turn=0` $\rightarrow$ P1: while loop
However, based on this timeline analysis, we see at the time of P1's while loop the most recent assignment to `wantCS[0]` was by P0, setting it to `true`, a contradiction. $\square$

#### Progress Property
Suppose at least one of P0 and P1 are waiting. Then the only reason P0 and P1 may not enter the CS is due to the while loop, since all other lines in `RequestCS` can be run in constant (in particular, bounded) time, i.e. it suffices to consider the case where a process is either
1. at the while loop if waiting, 
2. or completely outside `RequestCS` if not waiting.

Consider cases, where we split the cases by how many processes are waiting. The lecture slides seem to only consider the first case.

Case 1: Both processes are waiting.
Progress holds by the fact that it is not possible for `turn==0` and `turn==1` to both be true.

Case 2: Only P0 is waiting.
Since P1 is not waiting, P1 must either 
1. Amongst `RequestCS` and `ReleaseCS`, ran `ReleaseCS` more recently
2. Never ran `RequestCS` before
In either case, `wantCS[1]==false`, so that P0 can pass the while loop and enter the CS

Case 3: Symmetric to Case 2.

Hence progress. $\square$

#### No Starvation Property
WLOG, we show that if P0 is waiting, P0 will not be starved. It suffices to show that $\exists k\in\mathbb{N}$ such that at most $k$ *other* processes distinct from P0 (in this case P1) that can enter the CS before P0.


Note that we do not need to prove the case where P0 is the only process waiting, since progress implies P0 must be able to enter CS. Hence, we now prove bounded wait.

Similarly to the start of the proof of progress, we assume that P0 is at the while loop, where the loop condition evaluates to `true`. We claim that $k=1$ will suffice for the while loop to evaluate to false, allowing P0 to return from `RequestCS`.

When P0 is waiting, P0 will not make any changes to the `turn` variable. During this timeframe, P1 must first call `RequestCS` before entering CS, thereby setting `turn = 0`, in particular, P1 can only set change `turn` to `0`. The next time P1 calls `RequestCS` again, `turn == 0 && RequestCS[0] == true` so that P1 will block at the while loop. Hence, as long as P0 continues to run at this new timeframe, P0 can enter the CS.

Remark: It is actually not important whether P1 blocks or not (since this is relevant to mutual exclusion, but we are only talking about starvation here). As long as `turn == 0`, P0 will pass the while loop. $\square$

### Lamport's Bakery Algorithm
We note the following in the algorithm.
- Tuples `(queueNumber, id)` are ordered first by `queueNumber`, then `id`.
- In "get a number", $number[myid]\gets\max\{number[j] : 0\leq j<n\}+1$

#### Progress Property
Suppose a non-zero number of processes, say $1\leq k\leq n$ are waiting at `RequestCS`. Examining the code, we see that a process can only wait at one of the while loops in the second for loop.

Note that if a process is not waiting, i.e. $P\notin\{P_{i_1},\dots,P_{i_k}\}$, we can simply assume that it is completely outside `RequestCS` such that `choosing[process_id]==false` and `number[process_id]==0`.

Suppose a process, $P = P_i\in \{P_{i_1},\dots,P_{i_k}\}$ is one of the waiting processes. We consider cases.
1. P is waiting at `while (choosing[j]==true)` for some $j$. In this case, process $P_j$, which we will write as Pj can always complete the "get a number" portion, such that in a finite amount of time, `choosing[j]=false` executes. Then, `choosing[j]==true` evaluates to false under process P, or otherwise, we must have the assignment `choosing[j]=true`, which would mean Pj entered the CS, left the CS, and called `RequestCS`. While the 2nd case should not happen based on the code, even if it does, we already have progress.
2. After considering case 1, we can now assume that *all* waiting processes are now waiting at the second while loop, `while (number[j]!=0 && Smaller(number[j], j, number[i], i)`. In this case, we claim that we can find some $P_i\in \{P_{i_1},\dots,P_{i_k}\}$ such that $\forall j\in\{1,\dots,n\}$, `number[j]!=0 && Smaller(number[j], j, number[i], i)` evaluates to `true`, which we will prove below.

Remark: we can assume *"all waiting processes"* since there are finitely many processes, so that it takes a finite amount of time for all processes that will ever wait in this timeframe to be blocked at the 2nd while loop. 

**Lemma.** $\exists i\in\{1,\dots,k\}, \forall j\in\{1,\dots,n\}$, `number[j]!=0 && Smaller(number[j], j, number[i], i)` evaluates to true.

If $j\in \{1,\dots,n\}\setminus\{i_1,\dots,i_k\}$, then `number[j]==0` and so `number[j]!=0` evaluates to `false`.

Otherwise, amongst the elements `(number[index], index)` indexed by $\{i_1,\dots,i_k\}$, there must exist a minimal element, since there are finitely many elements. If $i_l$ indexes a minimal element, then process $P=P_{i_l}$ will have `Smaller(number[j], j, number[i_l], i_l)` evaluate to `false` for each $j\in\{1,\dots,n\}$.

Hence such a process P will pass both while loops at each iteration of the for loop and enter the CS. (in particular, we have shown that at least one waiting process can enter the CS.) $\square$

#### No Starvation Property
We fix a process of id $i\in\{1,\dots,n\}$, and that $P=P_i$ is waiting to enter the CS. We claim that P will not be starved.

As proven in progress, P will not be blocked by `while (choosing[j] == true)`.

At the current point of discussion, we sort the elements $\{(number[j], j) : 1\leq j\leq n \land number[j]\neq 0\}$. Then suppose $i$ is at position $p$. Then we claim that at most (in fact, exactly) $p-1$ processes can enter the CS before P does, i.e. $k = p-1$. (This is an inductive argument)

WLOG, we assume that all $p-1$ processes ranked ahead of $i$ by our ordering have entered and left the CS. At this point, regardless of what the other processes do, $(number[i], i) = \min\{(number[j], j) : 1\leq j\leq n \land number[j]\neq 0\}$, since "get a number" will grant any process entering `RequestCS` a queue number $\geq number[i] + 1$.

Hence, P will not be blocked by `while (number[j]!=0 && Smaller(number[j], j, number[i], i)` for any $j\in\{1,\dots,n\}$. $\square$

#### Mutual Exlusion Property
Suppose more than one process is in the CS. Choose any two of them, and label them P0, P1. Let the index of P0 be $k$ and the index of P1 be $l$. WLOG, $(number[k], k) < (number[l], l)$.

Consider the timeframe where P1 is at the second while loop, in particular, when P1 is comparing against P0. Since P1 passes this while loop, either `number[k] == 0` is true or `Smaller(number[k], k, number[l], l)` is false.
1. `Smaller(number[k], k, number[l], l)` is false: This case is not possible since $(number[k], k) < (number[l], l)$, furthermore even if P0 re-`RequestCS`, as long as P1's `number[i]` remains the same, the new number that P0 gets must still be strictly greater than `number[i]`.
2. `number[k] == 0`: At this timeframe, P0 must be before the line `number[myId]++`, since this line will result in `number[k] > 0`. Furthermore, P0 must be in the loop, with loop variable `j > l` (or more precisely, P0 has executed the check `if (number[j] > number[myId])` where `j==l`). If not, P0 will get a number strictly greater than `number[l]`.

Next, consider an earlier timeframe, where P1 is at the first while loop. We further note that P1 manages to pass the first while loop, so that `choosing[k] == false`. There is only 1 possibility for this to occur, this is because `choosing[k]` can only evaluate to true within the "get a number" section. P0 must be at some point strictly (chronologically) before the most recent invoation of the `choosing[myId] = true` line, this also takes into account earlier invocations of `RequestCS(0)` (e.g. where P0 is after `choosing[myId] = false`). We conclude that P0 must then evaluate `number[j] > number[myId]` with $j = l, myId = k$ and `number[k]`. Contradiction. $\square$

## Synchronization Primitives
Useful link: https://stackoverflow.com/questions/37026/java-notify-vs-notifyall-all-over-again

### Monitor (Java)
A process/thread can be in one of 3 states with regard to a synchronized method (monitor)
1. The thread is actively running. The monitor ensures that at most 1 thread can be in this state.
2. The thread is blocked. This can happen in 2 places.
   1. The thread calls the synchronized method and is attempting to acquire lock.
   2. The thread has been waken up from waiting state and is attempting to reacquire lock.
3. The thread is waiting, i.e. after a call to `wait`, the thread joins the waiting queue.

Useful link: https://stackoverflow.com/questions/5798637/is-it-safe-to-call-a-synchronized-method-from-another-synchronized-method

```java
void synchronized method1() {
    method2()
}

void synchronized method2() {
}
```

equivalent to 
```java
void method1() {
    synchronized (this) {
        method2()
    }
}

void method2() {
    synchronized (this) {

    }
}
```

Assume both methods are in the same class, so that `this` refers to the same object instance.
In a thread, when `method1` calls `method2`, the lock associated with `this` is already acquired, so the thread will not block.

### Producer/Consumer Problem
Pseudo-code from lecture slides
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isFull())
            sharedBuffer.wait();
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull();
        removeItemFrom(sharedBuffer);
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
We analyze this by cases.
#### One consumer, one producer
We claim that the solution is correct. We need to show that
1. When the buffer is full, producer does not write to it.
2. When the buffer is empty, consumer does not read from it.
3. Progress: Consumer can read from a non-empty buffer. Producer can write into a non-full buffer.

Suppose producer added an item to a full buffer, i.e. the producer executed
```java
        addItemTo(sharedBuffer);
```
when the buffer is full. (Call this time T1) We consider an earlier time point T2 where the producer most recently evaluated `sharedBuffer.isEmpty()`. Consider cases.
1. Suppose the evaluation is `false`, then somehow, an item got added into the buffer (between T2 and T1) without the producer having done anything. This is impossible, since only the producer is capable of adding items, and we only have one producer.
2. Suppose the evaluation is `true`, then the producer would enter the wait queue. The only way for the producer to wake up from the wait queue is if the consumer process calls `sharedBuffer.notify();`. We shall study whether it is possible for the buffer to still be full at this point in time.
   Denote T3 as the time the consumer calls `sharedBuffer.notify();`.
   Denote T4 as the time the consumer last calls `removeItemFrom(sharedBuffer);`. Note that T4 < T3 < T1. T2 is somewhere before T3.
   We claim that T2 < T4. Suppose not, then T4 < T2, but this says that $T4 < T2 < T3 < T1$, which implies that there are 2 processes inside the monitor-locked region, since consumer process is not blocked (since `removeItemFrom(sharedBuffer);` is strictly after the `wait`). Contradiction.
   Hence, $T2 < T4 < T3 < T1$. In other words, an item was removed from the buffer at T4, and the buffer remains untouched since then. In particular, the buffer is non-full.

We have a contradiction in both cases. Hence, it is impossible for a producer to produce into a full buffer.

Proving that the consumer does not consume from an empty buffer should be similar.

**Progress**
Next, we prove progress for producers. Claim: When the buffer is non-full, the producer can write to it in finite time, regardless of what the consumer does.
Clearly, if the producer is not blocked in the wait queue, then the producer can definitely produce for a non-full buffer. Hence, in order to obtain a contradiction, we assume that the producer is somehow in the wait queue despite a non-full queue.

Define T0 as the time point where the producer is in the wait queue, and *the buffer is not full*.
Define T1 as the time point where producer last executed `if (sharedBuffer.isFull())`. This must have evaluated to `true`, since the producer is now blocked.
So between T1 and T0, something must have happened to the buffer such that it is not full, i.e. a consumer removed an item from the buffer at some time T2, where $T1 < T2 < T0$.
Whether the consumer process then notifies the producer on the wait queue depends on whether `if (wasFull)` evaluates to `true`. Hence, define T3 as the time point where consumer executes `boolean wasFull = sharedBuffer.isFull();`. If $T1 < T3$ we are done.

Suppose that $T3 < T1$, this is not possible since we have 2 active processes inside a monitor.

```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isFull()) // T1
            sharedBuffer.wait(); // T0
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        if (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull(); // T3
        removeItemFrom(sharedBuffer); // T2
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
To be precise,
- producer is active in the time range $[T1, T0]$ and
- consumer is active at the time range $[T3, T2]$
- where $T1\in [T1, T0]\cap [T3, T2]$
there is an non-empty intersection in their active time periods, which contradicts the mutual exclusion guarantee of a monitor.

#### One consumer, multiple producers
A common point of failure with regard to the usage of monitors the that `notify` has no control over which process is woken up. Furthermore, the process that is woken up also competes with incoming processes.

We come up with a situation in which the synchronization mechanism fails.

Suppose the buffer size is $n\in \mathbb{Z}^+$. Assume that the buffer is full, after producers have written to it.
- 1 producer P1 gets blocked in the wait queue
- 1 consumer enters, removes item, and notifies.
- producer P1 wakes up, but does not manage to acquire monitor lock. Instead, an incoming producer P2 enters and acquires lock. P1 now waits in the monitor queue.
- P2 sees that `sharedBuffer.isFull() == true` and produces for the buffer. *Note that the buffer is now full again.*
- P2 exits monitor, P1 who is in the monitor queue then dequeues and adds another item to the queue. The algo therefore fails.

One particular point of failure is the use of `if` statement instead of `while`.

A similar case can be made for the multiple consumer, one producer case.

**Change**: Change the `if` to `while`
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isFull())
            sharedBuffer.wait();
        boolean wasEmpty = sharedBuffer.isEmpty();
        addItemTo(sharedBuffer);
        if (wasEmpty)
            sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        boolean wasFull = sharedBuffer.isFull();
        removeItemFrom(sharedBuffer);
        if (wasFull)
            sharedBuffer.notify();
    }
}
```
We now attempt a different way to make the algo fail. Consider the following sequence of events. Note that the buffer is of capacity $n$.
- There are $n+2$ producers. $n$ producers write to the buffer, the remaining $2$ producers waits in the queue.
- There are $2$ consumers. The first consumer wakes up a producer P1. But when P1 wakes up, it does not manage to acquire lock. Instead, the second consumer acquires lock first.
- Now, 1 producer P2 is in the monitor queue, despite the fact that the buffer is not full (since P1 produces 1 item, and the 2 consumers consume 2 items)

The point of failure is that `notify` only wakes 1 process up. (i.e. one of the 2 sleeping producers). Only the first consumer manages to call notify. The second one does not since buffer is no longer full.

**Change**: Change `notify` to `notifyAll`
This is probably correct. Idk.

**Change**: Remove the conditional
```java
Object sharedBuffer;

void produce() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isFull())
            sharedBuffer.wait();
        addItemTo(sharedBuffer);
        sharedBuffer.notify();
    }
}

void consume() {
    synchronized (sharedBuffer) {
        while (sharedBuffer.isEmpty())
            sharedBuffer.wait();
        removeItemFrom(sharedBuffer);
        sharedBuffer.notify();
    }
}
```
Idk. Too lazy. Think abt this next time.

### Reader-Writer problem
To prove correctness, we need to show the following:
1. When a writer is writing to a file F, no reader can read, and no other writer can write.
2. When (possibly multiple) readers are reading, writers cannot write.

More lemmas to prove:
- When reader calls `notify`, there can be no reader in the wait queue.

Extensions:
- Devise a no starvation algorithm.

```java
void writeFile() {
    synchronized (object) {
        while (numReader > 0 || numWriter > 0)
            object.wait();
        numWriter = 1;
    }
    // write to file;
    synchronized (object) {
        numWriter = 0;
        object.notifyAll();
    }
}

void readFile() {
 synchronized (object) {
        while (numWriter > 0)
            object.wait();
        numReader++;
    }
    // read from file;
    synchronized (object) {
        numReader--;
        object.notify();
    }
}

```

#### Correctness
We prove **1**. Suppose a writer W1 is writing to file F. Consider cases.
1. Suppose another writer W2 is writing to F as well. We define time point T1 where W1 most recently executes `while (numReader > 0 || numWriter > 0)`, and time point T2 such that W2 most recently executes `while (numReader > 0 || numWriter > 0)`. WLOG, $T1 < T2$. Then right before T2, `numWriter == 1`, since T2 can only occur when W1 has left the monitor block, so that `numWriter = 1` has executed. Hence, W2 will be blocked by the while loop and join the wait queue instead. Contradiction.
2. Suppose another reader R is reading from F. We define time point T1 where W1 most recently executes `while (numReader > 0 || numWriter > 0)` and time point T2 where R most recently executes `while (numWriter > 0)`. We consider cases.
   1. If $T1 < T2$, this is not possible since `numWriter == 1` at the time of T2 and reader R will get blocked.
   2. If $T2 < T1$, this is also not possible since `numReader++` has been executed by R by the time of T1. Hence, W1 gets blocked at T1. There is a small detail here. We need to show that `numReader` is always non-negative, so that when `numReader++` executes, it becomes positive. To show this, we observe that we can pair up the `++` and `--`.
   We have thus attained a contradiction in both subcases.
Hence, we have proven **1**. $\square$ 

We prove **2**. Suppose at least 1 reader R is reading, and a writer W is writing to F. This is actually the same as case 2 of **1**. So we have this proven for free.

**Lemma** When reader calls `notify`, there can be no reader in the wait queue.
Let R1 be the reader calling `notify` at time point T1 and we suppose that at T1 there exists a reader R2 in the wait queue.

```java
void writeFile() {
    synchronized (object) {
        while (numReader > 0 || numWriter > 0)
            object.wait();
        numWriter = 1;
    }
    // write to file;
    synchronized (object) {
        numWriter = 0;
        object.notifyAll();
    }
}

void readFile() {
 synchronized (object) {
        while (numWriter > 0) // T2: R2, T3: R1
            object.wait(); // R2 waiting
        numReader++;
    }
    // read from file;
    synchronized (object) {
        numReader--;
        object.notify(); // T1
    }
}
```
Let T2 be the time R2 most recently executed `while (numWriter > 0)` and let T3 be the time R1 most recently executed `while (numWriter > 0)`. We consider cases. Note that it is clear that $T2, T3 < T1$.
1. Suppose $T3 < T2 < T1$. At T3, `numWriter == 0` since R1 passes the while loop and at T2, `numWriter > 0` since R2 get blocked. Hence, this says that there is an incoming writer W in the time range $(T3, T2)$. Let T4 be the time W entered the monitor. We see that $T3 < T4 < T2$. W, seeing that `numReader > 0`, enters the wait queue and is unable to increment `numWriter`. The same holds even if more than 1 writer enters. Hence, it is impossible that `numWriter > 0`. Contradiction.
2. Suppose $T2 < T3 < T1$. At T2, `numWriter > 0` and at T3, `numWriter == 0`. Let W be a writer present in the time interval $(T2, T3)$. W writes and executes `notifyAll`. It is possible that another writer wakes up and acquires lock, but WLOG, we assume that W is the last writer to acquire lock **before T3**, i.e. after this it must be a reader that acquires lock. Let T4 be the time point W exits the monitor. At this point, no process can be in the wait queue, since all are woken up by `notifyAll`. In particular, R2 is awake. Since the next processes that acquire lock are all readers, this also includes R1. When R1 eventually reaches T1, there are no readers in the wait queue. This completes the proof. 
$\square$ 

**Corollary** If a reader notifies a process, that process must be a writer.

**Anti-Starvation** Done in homework document.

## Consistency Conditions
We use the following notation, if not otherwise specified
An event $e$ takes the form
$$\texttt{inv/resp(process, operation\_type, resource, value)}$$
- Value may be omitted if no value is involved in the operation

An operation $o$ is a pair of events $e, e'$, where $e$ denotes the invocation event and $e'$ denotes the response event. The properties associated with an operation are retrievable as follows:
- Invocation event $e = inv(o)$
- Response event $e' = resp(o)$
- Process $proc(o)$
- Resource $res(o)$

A history $H$ implies a partial order $<_H$, where $o_1 < o_2 \iff resp(o_1) < inv(o_2)$.
This is known as the external order, or occurred before order.

### Sequential history
Intuitively, a (legal) sequential history is one in which it seems like a single process is creating invoke events and receiving response events.

**Lemma** These 2 definitions of sequential history $(H, <_H)$ are equivalent.
1. $<_H$ is a total order on operations, such that for all operations $o_1, o_2$, $o_1 <_H o_2\lor o_2 <_H o_1$.
2. Every invocation of an operation $o$, $inv(o)$, must be immediately followed by its response, $resp(o)$, i.e. $H$ is of the form $inv(o_1), resp(o_1), inv(o_2), resp(o_2), \dots$

Clearly, **2** implies **1**. We shall only prove **1** implies **2**.
Suppose **1**. We then conduct induction on $H$, let $H$ have $n$ operations, so that the length of $H$ is $2n$. WLOG, let the first entry of $H$ be $inv(o_1)$. (Clearly, the first entry of $H$ cannot be a response event).
Suppose the second entry is not $resp(o_1)$, then it is another invocation event, say $inv(o_2)$. But this means that $o_1$ and $o_2$ are not comparable, since it is neither the case that $inv(o_2)$ follows $resp(o_1)$ nor is it the case that $inv(o_1)$ follows $resp(o_2)$. $\square$
#### Process order
A clearer definition of process order is the sub-partial order imposed by $\bigcup_{\text{process } p} <_{H|p}$.

A legal sequential history $S$ for a sequentially consistent history $H$ preserves process order, but not necessarily external order.
### Definitions of Linearizability
Claim: The 2 definitions of linearizability are equivalent.
Reformulation of definition 1: For a history $H = \{o_1,o_2,\dots,o_n\}$, where $o_i$ denotes an operation, $\exists$ choice function $c: \{1,2,\dots,n\}$ such that we have the corresponding "degenerate" history $\{c(o_i)\in [inv(o_i), resp(o_i)] : i\in \{1,2,\dots,n\}\}$, where for each $i$, $inv^*(o_i) = resp^*(o_i) = c(o_i)$, giving a degenerate legal sequential history $H^* = (inv^*(o_1), resp^*(o_1),inv^*(o_2), resp^*(o_2),\dots,inv^*(o_n), resp^*(o_n))$.

We call $H^*$ degenerate since it is technically not possible for events to take place at the same time.

Definition 2: A history $H$ for which there exists a legal sequential history $S$ that preserves the partial(external) order $<_H$ given by $H$. i.e. $<_H\subseteq <_S$.

Note the 3 parts of this definition: $S$ needs to be
1. legal
2. sequential
3. preserves $<_H$

$(\implies)$ Suppose definition 1 is true for a history $H$. We then construct a sequential history $S_c=([c(o_i) - \epsilon, c(o_i) + \epsilon])_{1\leq i\leq n}$ where each operation takes arbitrarily short time $\epsilon > 0$ **AND** $inv(o_i) \leq c(o_i) - \epsilon, c(o_i) + \epsilon \leq resp(o_i)$. Since there are finitely many intervals ($n$ of them), it is possible to choose small enough $\epsilon$ so that all intervals do not overlap. Since intervals are disjoint, $S_c$ is sequential.
Furthermore, each interval $[c(o_i) - \epsilon, c(o_i) + \epsilon]\subseteq [inv(o_i), resp(o_i)]$ in terms of time, so that the partial order $<_H$ is preserved. For example, if $a <_H b$ (equivalent to $resp(a) < inv(b)$), then $c(a) + \epsilon < c(b) - \epsilon$ so that $a <_S b$. Hence, the partial order is preserved by $S_c$.
Like the "degenerate" legal history(call it $D$), $S_c$ preserves the chronological order of events. In other words, for any operation $o$, the events that have occurred prior to $o$ in $S_c$ are exactly the same as in $D$, and in the same order too. The applies for what occurs after. Hence, the legality of $D$ implies the legality of $S_c$, as relative chronology is the same in $D$ and $S_c$.
Hence, we have show that definition 2 holds as well.

*Remark*: We cannot just say $D$ is a sequential history as technically speaking, no 2 events can occur at *exactly* the same time (we can treat this as an axiom), so there is no such thing as an operation with invocation time equalling response time.

$(\impliedby)$ Suppose definition 2 is true for a history $H$ with $n$ operations. Let $S$ be the legal sequential history with the properties mentioned (legal, sequential, preserves $<_H$). Without loss of generality, (by possibly renumbering the operations), $S$ gives the sequence $(o_1, o_2,\dots,o_n)$, where each $o_i$ subsumes a pair of $inv$ and $resp$ events. We then construct our choice function as follows. In fact, we choose points for operations in ascending order from $1$ to $n$.

At each $i$, let $d_i := \min\{resp(o_j) - \max\{c(o_{i-1}), inv(o_i)\} : j \geq i \}$, where we define $c(o_0):=-\infty$. Certainly $d_i > 0$. The reason:
- $c(o_{i-1}) < resp(o_j)$ by inductive hypothesis
- $inv(o_i) < resp(o_j)$ since $o_i <_S o_j$ for $j > i$ 

Then let $c(o_i) := \max\{c(o_{i-1}), inv(o_i)\} + \frac{d_i}{n}$. Intuitively, this gives us freedom to define the points $c(o_{i+1}) = c(o_i) + \frac{d_i}{n}$, $c(o_{i+2}) = c(o_{i+1}) + \frac{d_i}{n}$ if need be.
Formally, this allows us to re-establish the inductive hypothesis, as 
$$c(o_i) = \max\{c(o_{i-1}), inv(o_i)\} + \frac{d_i}{n} < \max\{c(o_{i-1}), inv(o_i)\} + d_i \leq \min\{resp(o_j) : j > i\}$$

We then need to show this is legal. But the legality of this comes directly from the legality of $S$, since the order of operations of the degenerate history $S_c$ exactly follows that of $S$, precisely stated as $<_{S_c} = <_S$. So we are done. $\square$

### Sequential consistency is not a local property
Given the events of history $H$ as follows:
1. inv(P, enqueue, x, 1) $p_1$
2. resp(P, enqueue, x)
3. inv(Q, enqueue, y, 2) $q_1$
4. resp(Q, enqueue, y)
5. inv(P, enqueue, y, 1) $p_2$
6. resp(P, enqueue, y)
7. inv(Q, enqueue, x, 2) $q_2$
8. resp(Q, enqueue, x)
9. inv(P, dequeue, x) $p_3$
10. inv(Q, dequeue, y) $q_3$
11. resp(P, dequeue, x, 2)
12. resp(Q, dequeue, y, 1)
where $p_i, q_i$ denote operations.

As seen in lecture, $H|x$ and $H|y$ are sequentially consistent. Now, we prove formally that $H$ is not sequentially consistent. Suppose not, such that there exists a legal sequential history $S$ for $H$.

Since $S$ must preserve process order, we must have $p_1 < p_2 < p_3\land q_1 < q_2 < q_3$ -- $(0)$. (Here $<$ denotes $<_S$)
- $p_1$ enqueues 1 into x, and $q_2$ enqueues 2 into x. Since $p_3$ dequeues 2 from x, we must have $q_2 < p_1$ -- $(1)$
- By a similar argument for y, we must have $p_2 < q_1$ -- $(2)$

We now have the following ordering which respects $(0)$ and $(1)$
$$q_1 < q_2 < p_1 < p_2 < p_3$$
But immediately we see a contradiction, since here, $q_1 < p_2$, yet $(2)$ states otherwise. This contradicts the fact that $S$ is a total order.

Hence $S$ cannot exist, that is, $H$ is not sequentially consistent.

### Linearizability is a local property
We try to prove it using Definition 1.

Given history $H$, suppose that it is locally linearizable.



We discuss some aspects of the proof in lecture.

There are 3 areas to discuss.












































\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{{./CS3230_images}}

\newcommand\tab[1][1cm]{\hspace*{#1}}

% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}


\title{CS3230 (Analysis of Algorithms)}
\author{Jia Cheng}
\date{September 2021}

\begin{document}

\maketitle

\section{Definitions and Formula}
\paragraph{Sum of geometric progression}
\begin{align*}
	\sum_{0\leq i\leq n}r^i = \frac{r^0-r^{n+1}}{1-r}=\frac{1-r^{n+1}}{1-r}
\end{align*}

\textbf{Importantly, remember that the top term is} $1-r^{n+1}$\textbf{, not} $1-r^n$\textbf{!!!!!!}

In general, for a geometric progression $a_1, a_2, \dots, a_n$ with common ratio $r$, its sum is \begin{align*}
	\frac{a_1 - a_n\cdot r}{1-r}
\end{align*}

\paragraph{Sum of arithmetic progression}
For an arithmetic progression $a_1, a_2, \dots, a_n$ with common difference $d$, its sum is
\begin{align*}
	\frac{n\cdot (a_1+a_n)}{2}
\end{align*}

\paragraph{Decision tree} Three terms \textbf{level}, \textbf{depth}, \textbf{height}\\
\includegraphics[scale=0.8]{tree.png}

Image from \texttt{https://stackoverflow.com/questions/63369552/depth-vs-level-of-a-tree}
\begin{itemize}
	\item For a complete binary tree, number of children at depth $k$ is $2^k$. In particular, the number of children at depth $0$ is $2^0=1$ (the root node).\\
	For example, in a comparison based sorting algorithm, the depth equals to the number of comparisons made.
	\item Height = Maximum depth of tree
	\item Level = Depth + 1
	\item Usually, depth is more useful than level since depth is $0$-indexed
\end{itemize}

\paragraph{Asymptotic notation} From wikipedia\\
\includegraphics[scale=0.5]{asymptotic_notation.png}


\paragraph{Upper and lower bounds} In algorithmic analysis, when we speak of upper and lower bounds, we are usually referring to the worst case. Of course, we tend to use bounds because we cannot quantify precisely what is the running time of an algorithm over all inputs.

For instance, if we know for a certain $n$, the runtime of algorithm $M$ over all inputs of size $n$ is $\{n,n+1,\dots,2n-1, 2n\}$, then $[2n, \infty)$ would all be valid upper bounds, and $[0, 2n]$ would be valid lower bounds. However, since we already know the worst case runtime: $2n$, there is no point talking about bounds.

Upper bounds require universal quantification. That is, to say that $U(n)$ is an upper bound on the runtime, we need $\forall$ inputs of size $n$, runtime is $\leq U(n)$.

In comparison, existential quantification is sufficient to justify a lower bound. To say that $\exists$ input of size $n$, runtime $\geq L(n)$, we simply need to provide an instance of an input that indeed results in $L(n)$ or more runtime.

The above can be described in terms of order theory. Let $S(n)$ be a set (representing the runtimes of size $n$ inputs).\\
The worst case runtime is analogous to $\sup S(n)$.

For a $n$ dependent value $U(n)$, $\sup S(n)\leq U(n) \iff \forall s\in S(n), s\leq U(n)$.

For a $n$ dependent value $L(n)$, $\sup S(n)\geq L(n) \iff \exists s\in S(n), s\geq L(n)$. This should be a very familiar statement in mathematical analysis. In particular, if $\exists s\in S(n), s=L(n)$, then indeed $L(n)\leq \sup S(n)$.

In conclusion, this asymmetry in algorithmic analysis is due to the fact that we only care about supremums and not infimums.

\section{Bounding Techniques}
\paragraph{Integral Bound}
For a monotonically increasing function $f$,
\begin{align*}
	\int_{i-1}^j f(x)\,dx \leq \sum_{x=i}^jf(x)\leq \int_i^{j+1}f(x)\,dx
\end{align*}

For a monotonically decreasing function $f$.
\begin{align*}
	\int_{i}^{j+1}f \leq \sum_{x=i}^jf(x) \leq \int_{i-1}^jf 
\end{align*}

\textit{Applications}
\begin{itemize}
	\item Harmonic series \\
	$H_n=\sum_{1\leq i\leq n}\frac{1}{i} = 1 + \sum_{2\leq i\leq n}\leq 1 + \int_1^{n}\frac{1}{i}\,di=1+\ln(n)=O(\log n)$\\
	$H_n\geq \int_1^{n+1}\frac{1}{i}\,di=\ln(n+1)=\Omega(\log n)$\\
	such that $H_n=\Theta(\log n)$
\end{itemize}

\paragraph{Telescoping}\mbox{}
Suppose we have the recursion $T(n) = aT(n/a) + nf(n)$. Then 
\begin{align*}
	\frac{T(n)}{n}=\frac{T(n/a)}{n/a}+f(n)
\end{align*}
and we can telescope on $S(n)=\frac{T(n)}{n}$

\paragraph{Stirling's Approximation}
\begin{align*}
	f(n)!&\sim \sqrt{2\pi f(n)}\left(\frac{f(n)}{e}\right)^{f(n)}\\
	\ln(f(n)!)&\sim f(n)\ln(f(n)) - f(n) = \Theta(f(n)\ln(f(n)))
\end{align*}


\paragraph{Logarithms}
\begin{align*}
	\lim_{n\rightarrow \infty}\frac{\log(\log(n)-k)}{\log(\log(n))} &= \lim_{x\rightarrow \infty}\frac{\ln(x-k)}{\ln(x)}\\
	&= \lim_{n\rightarrow \infty}\frac{\frac{1}{x-k}}{\frac{1}{x}}=1
\end{align*}
Hence $\log(\log(n)-k)\sim \log(\log(n))$


In general, define $f^n=f\circ f^{n-1}$, and
\begin{align*}
	\lim_{x\rightarrow \infty} \frac{\ln^n(x-k)}{\ln^n(x)} = \frac{\frac{1}{\ln^{n-1}(x-k)}\cdot\frac{1}{\ln^{n-2}(x-k)}\dots \frac{1}{\ln(x-k)}\cdot \frac{1}{x-k}}{\frac{1}{\ln^{n-1}(x)}\cdot\frac{1}{\ln^{n-2}(x)}\dots \frac{1}{\ln(x)}\cdot \frac{1}{x}}
\end{align*}
We can use strong induction to show that this limit is $1$.

\paragraph{Arithmetic sequence-like summations}
Consider a summation of the form
\begin{align*}
	\sum_{0\leq k\leq \frac{n}{m}}f(n-mk)
\end{align*}
where $m$ is a positive constant, and $f$ is a monotonically increasing function. If we expand out the summation, we realize that approximately $\frac{n}{2m}$ of the terms are greater than $\frac{n}{2}$.

To see this,
\begin{align*}
	n-mk\geq \frac{n}{2}\iff \frac{n}{2}\geq mk\iff k\leq \frac{n}{2m}
\end{align*}

Hence, we can consider the following \textbf{lower} bound
\begin{align*}
	\sum_{0\leq k\leq \frac{n}{m}}f(n-mk) \geq \sum_{0\leq k\leq \frac{n}{2m}}f(\frac{n}{2}) \approx \frac{n}{2m}\cdot f(\frac{n}{2})
\end{align*}
Usually such a bound is much tighter than simply taking the last term, $f(n)$, since we are taking into account a larger number of terms in the sum.

The no-brainer \textbf{upper} bound for such a sequence would be
\begin{align*}
	\sum_{0\leq k\leq \frac{n}{m}}f(n-mk) \leq \sum_{0\leq k\leq \frac{n}{m}}f(n)\approx \frac{n}{m}f(n) 
\end{align*}

\paragraph{Decision Tree}
The degree of branching that a decision tree based algorithm allows depends on the number of outcomes per step.\\
To be precise, suppose each step of an algorithm is able to partition the set of valid inputs into $k$ disjoint sets. Then we can think of this as branching factor $k$.

For e.g. if a single step in an algorithm allows 3 different types of outcomes, then the decision tree is ternary.

Suppose we can have $n$ possible results in total.\\
A decision tree with branching factor $k$ and height (maximum depth) $h$ has the following properties. Let $L$ be the number of leaves of the tree.
\begin{itemize}
	\item The leaves represent the outcomes. $L\leq 2^h$. Equality holds when the tree is complete.\\
	Example: A comparison based sorting algorithm has a branching factor of 2. Hence if $h$ comparisons are made, the tree has at most $2^h$ leaves. A necessary condition for the sorting algorithm to be correct (when there are $n$ elements) is that $L\geq n!$. Hence $2^h\geq n!$ and we have the familiar bound $h\geq \lg n!$
	\item The height of the tree represents the number of operations/"steps" that are taken. E.g. When $0$ operations are taken, the tree is of height $0$ and it only consists of $1$ leaf which is also the root node. Hence there are $2^0 = 1$ leaves.
\end{itemize}

When are decision trees ineffective in analysis?
\begin{itemize}
	\item Decision trees are good when \textbf{distinct inputs give distinct answers}. In other words, a correct algorithm is necessarily injective. Due to this property, if a decision tree is unable to produce as many leaves as the possible range of inputs, i.e. at least 1 leaf encompasses $>1$ input, then \textbf{the algorithm is unable to distinguish 2 distinct inputs with distinct solutions} and hence must be incorrect. 
	\item An example of incorrect usage of decision tree would be a simple query model problem. Taken from lecture 1,\\
	\textit{In the string query model, the input is a string of n bits. In one time unit, an algorithm can query one bit of the string. All other operations on the string can be performed at 0 cost. Let Zero be the problem of deciding whether an input string x is the all-zero string or not. The optimal algorithm for Zero has query complexity n.}\\
	There are $2^n$ valid inputs, since each entry $A[i]$ of an array $A$ can either be $1$ or $0$. Additionally, the decision tree is a binary one, since when the algo queries index $i\in \{1,\dots,n\}$, we either reply $A[i]=1$ or $A[i]=0$.\\
	However, even if the height of the decision tree is strictly less than $n$, such that there are strictly fewer than $2^n$ leaves, \textbf{we cannot immediately claim that the algo is incorrect}. Because it is certainly possible that the input $A[1]=A[2]=\dots=A[n]=0$ is in its own leaf, alone. The other leaves that encompass multiple inputs then all give the same answer (i.e. the string is not the All-Zero string). Then the adversarial argument would not work here.
\end{itemize}


\paragraph{Jensen's inequality for expected values}\mbox{}

Let $X$ be a random variable and let $f$ be a function.\\
If $f$ is concave (e.g. $x\mapsto \sqrt{x}$), then $E[f(X)]\leq f(E[X])$ (e.g. $E[\sqrt{X}]\leq \sqrt{E[X]}$) .\\
If $f$ is convex (e.g. $x\mapsto x^2$), then $E[f(X)]\geq f(E[X])$.\\
It is due to the convexity of the square function that $Var(X) = E[X^2]-E[X]^2\geq 0$


\section{Distributions}
\paragraph{Poisson approximation of Binomial Distribution}
Given binomially distributed $X\sim B(n, p)$, $X\approx Y\sim Poi(\lambda = np)$, where $Y\in \{0,1,2,\dots\}$

\begin{center}
	p.m.f. of $Y$: $P(Y=k) = e^{-\lambda}\frac{\lambda^k}{k!}$\\
	$P(Y=0) = \exp(-\lambda)$\\
	$P(X>0) \approx P(Y>0) = 1 - \exp(-\lambda)$
\end{center}


\section{Graphs}
Suppose a graph has vertices $V$ and edges $E$. Some common concepts associated with graphs (which we can use to argue or prove things are:
\begin{itemize}
	\item $V$
	\item Pairs of points ($V$ choose 2), adjacency matrix
	\item $E$
	\item Degree of vertex $v$, $deg(v)$
	\item Endpoints of edges
	\item Coloring nodes (this is a more general concept, and colors can often mean different things in different contexts)
\end{itemize}

\paragraph{Handshake Lemma}
\begin{align*}
	\sum_{1\leq i\leq |V|}deg(v_i) = 2\cdot |E|
\end{align*}

\section{Algorithms}
Themes

\paragraph{Similar subroutines to well-known algorithms}
\begin{itemize}
	\item Quickselect
	\item Partition
	\item Merge
	\item Radix sort (e.g. when items to sort, usually integers, are bounded above)
	\item Linear search
	\item Binary search (e.g. Searching the maximal element in a sorted, then cyclically rotated array)
\end{itemize}

\paragraph{Paradigms}
\begin{itemize}
	\item Divide and conquer
	\begin{itemize}
		\item Returning additional metadata (to improve asymptotic efficiency) e.g. O(n) version of maximal contiguous subarray sum/product
	\end{itemize}
	\item Streaming (e.g. find majority element)
\end{itemize}


\subsection{Dynamic Programming}
First, we have a rigorous formulation of dynamic programming. A usual DP recursive formula takes the form of 
\begin{align*}
	dp(n) = C + \max \{dp(i) : i\in S\}
\end{align*}
where $S\subseteq \{1,2,\dots n-1\}$ is a set of subproblems to be considered and $C$ is some constant. $\max$ can be replaced by any aggregate function, but we only consider maximization problems here for simplicity.

The usual proof of this equality involves the following conditions.
\begin{enumerate}
	\item $\exists i\in S, dp(n) = C + dp(i)$
	\item $\forall i\in S, dp(n)\geq C + dp(i)$
\end{enumerate}
When we argue that "Given an optimal solution $S_n$ to the problem with input $n$, $S_n$ contains within it an optimal solution $S_i$, for some $i\in S$.", we are demonstrating condition (1).\\
Condition (2) can be show using cut and paste. A way to justify (2) is to show that every solution to a subproblem $S_i$ can be built up to a solution $S'_n$ with value $dp(i) + C$, and by optimality of $S_n$, $S'_n$ must be inferior, such that $dp(i) + C\leq dp(n)$. Usually, cut and paste (in CS3230) is limited to showing optimality of a specific subproblem $S_i$ and not the full extent of condition (2).

(1) implies that $dp(n)\leq C + \max \{dp(i) : i\in S\}$, since $dp(n)\leq C + dp(i)$.\\
(2) implies that $dp(n)\geq C + \max \{dp(i) : i\in S\}$.
Hence we have the two way bound
\begin{align*}
	C + \max \{dp(i) : i\in S\}\leq dp(n)\leq C + \max \{dp(i) : i\in S\}
\end{align*}
which proves the recursive relation.

\subparagraph{Greedy} The greedy equivalent of the DP formula would be
\begin{align*}
	g(n) = C + g(f(n))
\end{align*}
where $f(n)\in \{1,2,\dots, n-1\}$ is a particular subproblem to be considered. $f$ is the greedy choice function.

Greedy choice property: There exists an optimal solution making a particular choice, which can be understood as showing that we can define $f(n)$.\\
Optimal substructure (for this particular choice): $C + g(f(n))\leq g(n)\leq C + g(f(n))$.

\paragraph{Subproblem Graph} CLRS Pg 367, 380-381

Looking at the graph, the runtime of a memoized algorithm is the number of directed edges. (Notice that each edge is "traversed" exactly once). Hence, by summing the outdegrees of each vertex, we can obtain the overall runtime.


The edges represent calling the subprocedures, but not "stepping into" them.\\
The outdegree of a vertex is usually (but not always) the runtime of a single call to the procedure without further recursion. \\
If the procedure itself has something additional computation involved internally, then we will also need to factor this into the runtime. This can be represented by the vertex itself.\\
Combining the above 2 points, we need to add up the edges and the vertices.

Hence the runtime of a memoized procedure, say $func$ is $T(n) = O(\sum_{1\leq i\leq n}S(i))$, where $S(i)$ is the runtime of a call to $func(i)$ without going into the recursive calls.

\paragraph{A loose upper bound} Adapted from CLRS Pg 380
There are 2 factors that determine the running time of a DP solution
\begin{itemize}
	\item (1) how many subproblems an optimal solution to the original problem uses,
	\item (2) how many choices we have in determining which subproblem(s) to use in an
optimal solution
\end{itemize}

A (possibly loose) upper bound for the DP solution would then be the product of (1) and (2).

\subsection{Network flow}
\paragraph{Terminology}
\begin{itemize}
	\item Network Graph $G=(V,E)$ with capacity function $c=(u,v)\mapsto \mathbb{R}$
	\begin{itemize}
		\item Simplication: Antiparallel property, $(u,v)\in E\implies (v,u)\notin E$
		\item $(u,v)\notin E\implies c(u,v) = 0$
	\end{itemize}
	\item Flow $f:V\times V\rightarrow \mathbb{R}$, $f=(u,v)\mapsto f(u,v)$ where $f(u,v)$ abides by 2 constraints
	\begin{itemize}
		\item Capacity upper bound: $0\leq f(u,v)\leq c(u,v)$
		\item Conservation: $\sum_{v\in V}f(u,v) = \sum_{v\in V}f(v,u)$
	\end{itemize}
	\item Value of a flow, $|f|=\sum_{v\in V}f(s,v)-\sum_{v\in V}f(v,s)$. Note that in a graph without antiparallel edges, the second term is always $0$. However, we would also like to consider flow of residual graphs, which can have edges from vertices $v$ to source $s$. Furthermore, an augmenting flow $f'$ can have negative value since this does not go against the definition. (Of course, in practice, we do not make use of augmenting flows with non-positive value. In particular, this is always possible according to lemma 26.2 of CLRS.)
	\item Residual graph augmented by flow $f$, $G_f$
	\begin{itemize}
		\item Residual capacity function $c_f$, given by
		$c_f(u,v)=\begin{cases}c(u,v) - f(u,v) \text{ if } (u,v)\in E\\ f(v,u)\text{ if } (v,u)\in E\\ 0 \text{ otherwise} \end{cases}$\\
		In particular $c_f(u,v)$ is the residual capacity of an edge $(u,v)$.
		\item Edge set of residual graph $E_f=\{(u,v)\in V\times V : c_f(u,v)>0\}$
		\item Augmenting path $p=s\leadsto t$ in $G_f$
		\item Residual capacity of path $p$, $c_f(p)=\min \{c_f(u,v) : (u,v)\in p\}$
		\item Augmentation of flow $f$ by $f'$, 
		$(f\uparrow f')(u,v)=\begin{cases}
			f(u,v) + f'(u,v) - f'(v,u) \text{ if } (u,v)\in E\\
			0 \text{ otherwise}		
		\end{cases}$
	\end{itemize}
\end{itemize}

Note that lemma 26.2 states that for a path $p$ in $G_f$, the flow  $f_p$, defined by
\begin{align*}
	f_p(u,v)=\begin{cases}
		c_f(p) \text{ if } (u,v)\in p\\
		0 \text{ otherwise}
	\end{cases}
\end{align*}
is such that $|f\uparrow f_p| = |f| + |f_p| > |f|$ since $c_f(p)>0$ (we only allow edges with positive weights in $E_f$).

\section{Reductions}
Let $\Sigma, \Gamma$ be 2 alphabets. For 2 decision problems $A\subseteq \Sigma^*$, $B\subseteq \Gamma^*$, $A\leq_p B$ iff there exists a polynomial time computable function $f:\Sigma^*\rightarrow \Gamma^*$ such that $a\in A\iff b=f(a)\in B$.

This is a particular case of a more general concept of many-to-one reduction $\leq_m$.

\end{document}


